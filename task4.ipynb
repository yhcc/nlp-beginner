{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch as T\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "USE_CUDA = T.cuda.is_available()\n",
    "\n",
    "LongTensor = T.cuda.LongTensor if USE_CUDA else T.LongTensor\n",
    "FloatTensor = T.cuda.FloatTensor if USE_CUDA else T.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Precision, recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label      Precision  Recall     F1         Support   \n",
      "ORG        0.71       0.84       0.77       1479.00   \n",
      "O          0.99       0.97       0.98       42674.00  \n",
      "MISC       0.75       0.87       0.80       949.00    \n",
      "PER        0.78       0.95       0.85       2442.00   \n",
      "LOC        0.87       0.90       0.88       1816.00   \n",
      "\n",
      "Avg        0.82       0.90       0.86       9872.00   \n"
     ]
    }
   ],
   "source": [
    "generate_confusion_matrix(flatten(predicts), flatten(val_ys), index2tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Example from validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: yeltsin has said any deal should <unknown> russia 's <unknown> <unknown> .\n",
      "Ground true: PER O O O O O O LOC O O O O\n",
      "Prediction: PER O O O O O O LOC O O O O\n",
      "Score: 153.1388\n"
     ]
    }
   ],
   "source": [
    "randomDemo(model, val_Xs, val_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hot map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdYAAAHICAYAAADzxsuxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd7001df10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_hot_map(flatten(predicts), flatten(val_ys), index2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readConll(filename):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        sentences: list. [[],[],...]\n",
    "        tags: list: [[],[]]\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    with open(filename) as f:\n",
    "        #drop first two lines\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        sentence = []\n",
    "        tag = []\n",
    "        while line:\n",
    "            line = line.strip()\n",
    "            if len(line)==0:\n",
    "                sentences.append(sentence)\n",
    "                tags.append(tag)\n",
    "                sentence = []\n",
    "                tag = []\n",
    "            else:\n",
    "                s, t = line.split('\\t')\n",
    "                s = s.lower()\n",
    "                sentence.append(s)\n",
    "                tag.append(t)\n",
    "            line = f.readline()\n",
    "    return sentences, tags\n",
    "\n",
    "def _to_ix(arr, count=False):\n",
    "    \"\"\"\n",
    "    arr:[[],[]..]\n",
    "    \"\"\"\n",
    "    to_ix = {}\n",
    "    counter = {}\n",
    "    for ar in arr:\n",
    "        for c in ar:\n",
    "            if not to_ix.has_key(c):\n",
    "                to_ix[c] = len(to_ix)\n",
    "                counter[c] = 1\n",
    "            else:\n",
    "                counter[c] += 1\n",
    "    if count:\n",
    "        return to_ix, counter\n",
    "    else:\n",
    "        return to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences, train_tags = readConll('train.conll')\n",
    "val_sentences, val_tags = readConll('dev.conll')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words before trim: 21010\n",
      "Num of words after trim: 10952\n"
     ]
    }
   ],
   "source": [
    "START_TAG = 'START'\n",
    "END_TAG = 'END'\n",
    "\n",
    "MIN_OCC = 2 #出现次数低于这个数字，则删除该单词\n",
    "word2index = {'<pad>':0, '<unknown>':1}\n",
    "\n",
    "_, wordcounter = _to_ix(train_sentences, count=True)\n",
    "print \"Num of words before trim: %d\"%(len(wordcounter))\n",
    "for k, v in wordcounter.items():\n",
    "    if v>=MIN_OCC:\n",
    "        word2index[k] = len(word2index)\n",
    "        \n",
    "print \"Num of words after trim: %d\"%(len(word2index))\n",
    "\n",
    "tag2index = _to_ix(train_tags)\n",
    "tag2index[START_TAG] = len(tag2index)\n",
    "tag2index[END_TAG] = len(tag2index)\n",
    "\n",
    "index2tag = {v:k for k,v in tag2index.items()}\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 1], [3]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2], [3]]\n",
    "b = np.copy(a)\n",
    "b[0].append(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert sentence to list\n",
    "from copy import deepcopy\n",
    "\n",
    "def prepare_sequence(seqs, to_ix):\n",
    "    res = []\n",
    "    for seq in seqs:\n",
    "        r = []\n",
    "        for w in seq:\n",
    "            if to_ix.has_key(w):\n",
    "                r.append(to_ix[w])\n",
    "            else:\n",
    "                r.append(to_ix['<unknown>'])\n",
    "        res.append(r)\n",
    "    return np.array(res)\n",
    "\n",
    "def ix_to_c(seq, ix_to):\n",
    "    res = []\n",
    "    for c in seq:\n",
    "        res.append(ix_to[c])\n",
    "    return \" \".join(res)\n",
    "\n",
    "def get_batch(sentences, tags, batch_size):\n",
    "    #sentences = np.copy(sentences)\n",
    "    #tags = np.copy(tags)\n",
    "    indexes = np.arange(len(sentences))\n",
    "    random.shuffle(indexes)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex<len(sentences):\n",
    "        batch_idx = indexes[sindex:eindex]\n",
    "        batch_X = sentences[batch_idx]\n",
    "        tmp = zip(batch_idx, batch_X)\n",
    "        tmp = sorted(tmp, key=lambda x:len(x[1]), reverse=True)\n",
    "        sort_indexes = [x[0] for x in tmp]\n",
    "        batch_X = deepcopy(sentences[sort_indexes])\n",
    "        batch_y = deepcopy(tags[sort_indexes])\n",
    "        max_len = len(batch_X[0])\n",
    "        seq_len = []\n",
    "        for i in xrange(len(batch_X)):\n",
    "            seq_len.append(len(batch_X[i]))\n",
    "            while len(batch_X[i])<max_len:\n",
    "                batch_X[i].append(word2index['<pad>'])\n",
    "                batch_y[i].append(batch_y[i][-1])\n",
    "        yield batch_X, batch_y, seq_len\n",
    "        sindex = eindex\n",
    "        eindex += batch_size\n",
    "    if sindex<len(sentences):\n",
    "        batch_X = sentences[batch_idx]\n",
    "        tmp = zip(batch_idx, batch_X)\n",
    "        tmp = sorted(tmp, key=lambda x:len(x[1]), reverse=True)\n",
    "        sort_indexes = [x[0] for x in tmp]\n",
    "        batch_X = deepcopy(sentences[sort_indexes])\n",
    "        batch_y = deepcopy(tags[sort_indexes])\n",
    "        max_len = len(batch_X[0])\n",
    "        seq_len = []\n",
    "        for i in xrange(len(batch_X)):\n",
    "            seq_len.append(len(batch_X[i]))\n",
    "            while len(batch_X[i])<max_len:\n",
    "                batch_X[i].append(word2index['<pad>'])\n",
    "                batch_y[i].append(batch_y[i][-1])\n",
    "        yield batch_X, batch_y, seq_len\n",
    "        sindex = eindex     \n",
    "        \n",
    "        \n",
    "def get_order_sample(sentences):\n",
    "    indexes = list(xrange(len(sentences)))\n",
    "    for index in indexes:\n",
    "        sentence = sentences[index]\n",
    "        seq_len = [len(sentence)]\n",
    "        yield Variable(LongTensor(sentence).view(1, -1)), seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_Xs = prepare_sequence(train_sentences, word2index)\n",
    "train_ys = prepare_sequence(train_tags, tag2index)\n",
    "\n",
    "val_Xs = prepare_sequence(val_sentences, word2index)\n",
    "val_ys = prepare_sequence(val_tags, tag2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(vec):\n",
    "    \"\"\"\n",
    "    vec: BxT->Bx1\n",
    "    \"\"\"\n",
    "    if len(vec.size())>1:\n",
    "        max_value = vec.max(1)[0].view(-1, 1) #B,\n",
    "        res = max_value + T.log(T.sum(T.exp(vec-max_value), dim=1, keepdim=True))\n",
    "    else:\n",
    "        max_value = vec.data.max()\n",
    "        res = max_value + T.log(T.sum(T.exp(vec-max_value)))\n",
    "    return res\n",
    "    \n",
    "\n",
    "class BiLSTM_RCF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag2index, embedding_size, hidden_size):\n",
    "        super(BiLSTM_RCF, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.tag_size = len(tag2index)\n",
    "        self.tag2index = tag2index\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        #transitions, x方向是如果转到该index，transition的值\n",
    "        self.transitions = nn.Parameter(T.randn(self.tag_size, self.tag_size))\n",
    "         \n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size//2, batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size, self.tag_size)\n",
    "        \n",
    "        #不允许转到start tag\n",
    "        self.transitions.data[tag2index[START_TAG], :].fill_(-10000)\n",
    "        #不允许从end tag转出来\n",
    "        self.transitions.data[:, tag2index[END_TAG]].fill_(-10000)\n",
    "        \n",
    "    def _init_hiddens(self, batch_size):\n",
    "        h = Variable(T.zeros(2, batch_size, self.hidden_size//2).type(FloatTensor))\n",
    "        c = Variable(T.zeros(2, batch_size, self.hidden_size//2).type(FloatTensor))\n",
    "        return (h, c)\n",
    "    \n",
    "    def _forward_alg(self, feats, seq_len):\n",
    "        \"\"\"\n",
    "        feats: BxLxT\n",
    "        \"\"\"\n",
    "        B = feats.size(0)\n",
    "        L = feats.size(1)\n",
    "        forward_var = T.ones(B, self.tag_size).type(FloatTensor)*(-10000)\n",
    "        forward_var[:, self.tag2index[START_TAG]] = 0\n",
    "        \n",
    "        forward_var = Variable(forward_var)\n",
    "        \n",
    "        seq_len_ = Variable(T.LongTensor(seq_len))\n",
    "        bindex = LongTensor(list(xrange(feats.size(0))))\n",
    "        for f_idx in xrange(L):\n",
    "            next_scores = []\n",
    "            mask = seq_len_.gt(f_idx).type(FloatTensor).view(B, 1)\n",
    "            #unmask = seq_len_.lt(f_idx).type(FloatTensor).view(B, 1)\n",
    "            for tag_idx in xrange(self.tag_size):\n",
    "                tags = LongTensor([tag_idx]*B)\n",
    "                emis_score = feats[bindex, LongTensor([f_idx]), tags].view(B, 1) #B, \n",
    "                tran_score = self.transitions[tags] #BxT\n",
    "                score = emis_score + tran_score + forward_var\n",
    "                score = log_sum_exp(score)*mask + forward_var[:, tag_idx].contiguous().view(-1, 1)*(1-mask) #B,1\n",
    "                next_scores.append(score)\n",
    "            forward_var = T.cat(next_scores, dim=1) #BxT\n",
    "        end_tags = LongTensor([self.tag2index[END_TAG]]*B)\n",
    "        final_score = forward_var + self.transitions[end_tags] #BxT\n",
    "        \n",
    "        return log_sum_exp(final_score) #B,1\n",
    "    \n",
    "    def _score_sentence(self, feats, tags, seq_len):\n",
    "        seq_len_ = Variable(LongTensor(seq_len))\n",
    "        score = Variable(T.zeros(feats.size(0),).type(FloatTensor))\n",
    "        previous_tag = LongTensor([self.tag2index[START_TAG]]*feats.size(0))\n",
    "        bindex = LongTensor(list(xrange(feats.size(0))))\n",
    "        for i in xrange(feats.size(1)):\n",
    "            mask = seq_len_.gt(i).type(FloatTensor)\n",
    "            tag = tags[:, i].data #B,\n",
    "            tran_score = self.transitions[tag, previous_tag] #B,\n",
    "            emis_score = feats[bindex, LongTensor([i]), tag] #B,\n",
    "            previous_tag = tag\n",
    "            score += (emis_score + tran_score)*mask\n",
    "        ends_tags = LongTensor([self.tag2index[END_TAG]]*feats.size(0))\n",
    "        score += self.transitions[ends_tags, previous_tag]\n",
    "        return score\n",
    "    \n",
    "    def _get_lstm_features(self, sentence, seq_len):\n",
    "        x = self.embedding(sentence) #B x L x D\n",
    "        hiddens = self._init_hiddens(x.size(0))\n",
    "        \n",
    "        pack = nn.utils.rnn.pack_padded_sequence(x, seq_len, batch_first=True)\n",
    "        \n",
    "        output, hiddens = self.lstm(pack, hiddens) #B x L x H\n",
    "        \n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        #output: B x L x H\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        output = self.fc(output)\n",
    "        return output # (B*L)xT\n",
    "    \n",
    "    def calculate_loss(self, sentence, tags, seq_len):\n",
    "        \"\"\"\n",
    "        tag padding重复最后一位，因为计算需要用到\n",
    "        \"\"\"\n",
    "        feats = self._get_lstm_features(sentence, seq_len)\n",
    "        feats = feats.view(sentence.size(0), sentence.size(1), self.tag_size) #BxLxT\n",
    "        numerator = self._score_sentence(feats, tags, seq_len) \n",
    "        denominator = self._forward_alg(feats, seq_len)\n",
    "        \"negative log likelihood\"\n",
    "        \n",
    "        return T.mean(denominator - numerator)\n",
    "    \n",
    "    def viterbi_decode(self, sentence, seq_len):\n",
    "        feats = self._get_lstm_features(sentence, seq_len)\n",
    "        \n",
    "        feats = feats.cpu().data.numpy()\n",
    "        forward_var = np.ones(self.tag_size)*(-10000.0)\n",
    "        forward_var[self.tag2index[START_TAG]] = 0.0\n",
    "        \n",
    "        previous_tag = 0\n",
    "        paths = []\n",
    "        for feat in feats:\n",
    "            path = []\n",
    "            scores = np.zeros(self.tag_size)\n",
    "            for i in xrange(self.tag_size):\n",
    "                tran_score = self.transitions[i]\n",
    "                score = tran_score.cpu().data.numpy() + forward_var\n",
    "                index = np.argmax(score)\n",
    "                scores[i] = max(score)\n",
    "                path.append(index)\n",
    "            forward_var = scores + feat\n",
    "            paths.append(path)\n",
    "        \n",
    "        final_score = forward_var + self.transitions[self.tag2index[END_TAG]].cpu().data.numpy()\n",
    "        \n",
    "        index = np.argmax(final_score)\n",
    "        \n",
    "        decode_paths = [index]\n",
    "        for i in xrange(len(paths)-1, 0, -1):\n",
    "            index = paths[i][index]\n",
    "            decode_paths.append(index)\n",
    "            \n",
    "        return max(final_score), list(reversed(decode_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "hidden_size = 128\n",
    "\n",
    "model = BiLSTM_RCF(len(word2index), tag2index, embedding_size, hidden_size)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "[1/20] loss:9.3583 time_elapse:0m38s\n",
      "[1/20] loss:4.2603 time_elapse:1m15s\n",
      "[1/20] loss:2.9628 time_elapse:1m53s\n",
      "[1/20] loss:2.2301 time_elapse:2m30s\n",
      "[1/20] loss:4.2820 time:2m56s\n",
      "----------\n",
      "[2/20] loss:1.2512 time_elapse:3m34s\n",
      "[2/20] loss:1.2174 time_elapse:4m14s\n",
      "[2/20] loss:1.1042 time_elapse:4m52s\n",
      "[2/20] loss:1.0427 time_elapse:5m29s\n",
      "[2/20] loss:1.1251 time:5m55s\n",
      "----------\n",
      "[3/20] loss:0.5967 time_elapse:6m33s\n",
      "[3/20] loss:0.5696 time_elapse:7m11s\n",
      "[3/20] loss:0.6920 time_elapse:7m49s\n",
      "[3/20] loss:0.6619 time_elapse:8m26s\n",
      "[3/20] loss:0.6323 time:8m52s\n",
      "----------\n",
      "[4/20] loss:0.4003 time_elapse:9m30s\n",
      "[4/20] loss:0.3898 time_elapse:10m8s\n",
      "[4/20] loss:0.4154 time_elapse:10m45s\n",
      "[4/20] loss:0.4165 time_elapse:11m22s\n",
      "[4/20] loss:0.4100 time:11m48s\n",
      "----------\n",
      "[5/20] loss:0.2686 time_elapse:12m27s\n",
      "[5/20] loss:0.2513 time_elapse:13m5s\n",
      "[5/20] loss:0.2257 time_elapse:13m41s\n",
      "[5/20] loss:0.3144 time_elapse:14m18s\n",
      "[5/20] loss:0.2767 time:14m44s\n",
      "----------\n",
      "[6/20] loss:0.1924 time_elapse:15m22s\n",
      "[6/20] loss:0.1799 time_elapse:15m59s\n",
      "[6/20] loss:0.2541 time_elapse:16m35s\n",
      "[6/20] loss:0.2788 time_elapse:17m12s\n",
      "[6/20] loss:0.2286 time:17m38s\n",
      "----------\n",
      "[7/20] loss:0.1410 time_elapse:18m15s\n",
      "[7/20] loss:0.1779 time_elapse:18m53s\n",
      "[7/20] loss:0.1818 time_elapse:19m31s\n",
      "[7/20] loss:0.2061 time_elapse:20m7s\n",
      "[7/20] loss:0.1902 time:20m33s\n",
      "----------\n",
      "[8/20] loss:0.1682 time_elapse:21m10s\n",
      "[8/20] loss:0.1612 time_elapse:21m47s\n",
      "[8/20] loss:0.1695 time_elapse:22m25s\n",
      "[8/20] loss:0.1942 time_elapse:23m2s\n",
      "[8/20] loss:0.1752 time:23m28s\n",
      "----------\n",
      "[9/20] loss:0.1123 time_elapse:24m5s\n",
      "[9/20] loss:0.1185 time_elapse:24m42s\n",
      "[9/20] loss:0.1345 time_elapse:25m20s\n",
      "[9/20] loss:0.1755 time_elapse:25m57s\n",
      "[9/20] loss:0.1441 time:26m23s\n",
      "----------\n",
      "[10/20] loss:0.1059 time_elapse:27m1s\n",
      "[10/20] loss:0.1502 time_elapse:27m39s\n",
      "[10/20] loss:0.1571 time_elapse:28m17s\n",
      "[10/20] loss:0.1719 time_elapse:28m55s\n",
      "[10/20] loss:0.1600 time:29m21s\n",
      "----------\n",
      "[11/20] loss:0.1164 time_elapse:29m58s\n",
      "[11/20] loss:0.1181 time_elapse:30m36s\n",
      "[11/20] loss:0.1002 time_elapse:31m15s\n",
      "[11/20] loss:0.0973 time_elapse:31m52s\n",
      "[11/20] loss:0.1050 time:32m17s\n",
      "----------\n",
      "[12/20] loss:0.0368 time_elapse:32m55s\n",
      "[12/20] loss:0.0248 time_elapse:33m32s\n",
      "[12/20] loss:0.0338 time_elapse:34m9s\n",
      "[12/20] loss:0.0328 time_elapse:34m47s\n",
      "[12/20] loss:0.0340 time:35m13s\n",
      "----------\n",
      "[13/20] loss:0.0180 time_elapse:35m51s\n",
      "[13/20] loss:0.0255 time_elapse:36m28s\n",
      "[13/20] loss:0.0191 time_elapse:37m6s\n",
      "[13/20] loss:0.0220 time_elapse:37m43s\n",
      "[13/20] loss:0.0213 time:38m8s\n",
      "----------\n",
      "[14/20] loss:0.0189 time_elapse:38m46s\n",
      "[14/20] loss:0.0161 time_elapse:39m24s\n",
      "[14/20] loss:0.0160 time_elapse:40m1s\n",
      "[14/20] loss:0.0167 time_elapse:40m38s\n",
      "[14/20] loss:0.0168 time:41m4s\n",
      "----------\n",
      "[15/20] loss:0.0123 time_elapse:41m41s\n",
      "[15/20] loss:0.0167 time_elapse:42m19s\n",
      "[15/20] loss:0.0114 time_elapse:42m56s\n",
      "[15/20] loss:0.0123 time_elapse:43m34s\n",
      "[15/20] loss:0.0132 time:44m1s\n",
      "----------\n",
      "[16/20] loss:0.0079 time_elapse:44m39s\n",
      "[16/20] loss:0.0122 time_elapse:45m19s\n",
      "[16/20] loss:0.0117 time_elapse:45m57s\n",
      "[16/20] loss:0.0172 time_elapse:46m34s\n",
      "[16/20] loss:0.0119 time:47m1s\n",
      "----------\n",
      "[17/20] loss:0.0075 time_elapse:47m39s\n",
      "[17/20] loss:0.0161 time_elapse:48m17s\n",
      "[17/20] loss:0.0113 time_elapse:48m54s\n",
      "[17/20] loss:0.0089 time_elapse:49m32s\n",
      "[17/20] loss:0.0108 time:49m58s\n",
      "----------\n",
      "[18/20] loss:0.0127 time_elapse:50m35s\n",
      "[18/20] loss:0.0117 time_elapse:51m12s\n",
      "[18/20] loss:0.0106 time_elapse:51m50s\n",
      "[18/20] loss:0.0069 time_elapse:52m28s\n",
      "[18/20] loss:0.0110 time:52m54s\n",
      "----------\n",
      "[19/20] loss:0.0087 time_elapse:53m31s\n",
      "[19/20] loss:0.0097 time_elapse:54m9s\n",
      "[19/20] loss:0.0070 time_elapse:54m45s\n",
      "[19/20] loss:0.0109 time_elapse:55m23s\n",
      "[19/20] loss:0.0097 time:55m48s\n",
      "----------\n",
      "[20/20] loss:0.0074 time_elapse:56m26s\n",
      "[20/20] loss:0.0106 time_elapse:57m4s\n",
      "[20/20] loss:0.0132 time_elapse:57m43s\n",
      "[20/20] loss:0.0163 time_elapse:58m19s\n",
      "[20/20] loss:0.0117 time:58m44s\n"
     ]
    }
   ],
   "source": [
    "trainIter(model, train_Xs, train_ys, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "T.save(model.state_dict(), 'params.pkl')\n",
    "#model.load_state_dict(T.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_elapse(since):\n",
    "    now = time.time()\n",
    "    ts = now - since\n",
    "    return \"{:.0f}m{:.0f}s\".format(ts//60, ts%60)\n",
    "\n",
    "def trainIter(model, train_Xs, train_ys, optimizer, scheduler=None, num_epochs=20, batch_size=32, \n",
    "              print_every=100):\n",
    "    since = time.time()\n",
    "    for epoch in xrange(num_epochs):\n",
    "        print '-'*10\n",
    "        if scheduler!=None:\n",
    "            scheduler.step()\n",
    "        epoch_loss = 0.0\n",
    "        print_loss = 0.0\n",
    "        print_idx = 0\n",
    "        for X, y, seq_len in get_batch(train_Xs, train_ys, batch_size):\n",
    "            X = Variable(LongTensor(X))\n",
    "            y = Variable(LongTensor(y))\n",
    "            loss = model.calculate_loss(X, y, seq_len)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.data[0]\n",
    "            print_loss += loss.data[0]\n",
    "            \n",
    "            print_idx += 1\n",
    "            \n",
    "            if print_idx%print_every==0:\n",
    "                print \"[{}/{}] loss:{:.4f} time_elapse:{}\".format(epoch+1, num_epochs, \n",
    "                                                                        print_loss/print_idx, \n",
    "                                                                        time_elapse(since))\n",
    "                print_idx = 0\n",
    "                print_loss = 0.0\n",
    "            \n",
    "        print \"[{}/{}] loss:{:.4f} time:{}\".format(epoch+1, num_epochs, epoch_loss/len(train_Xs)*batch_size,\n",
    "                                                   time_elapse(since))\n",
    "        \n",
    "def randomDemo(model, val_Xs, val_ys):\n",
    "    index = random.randint(0, len(val_Xs))\n",
    "    sentence = val_Xs[index]\n",
    "    tag = val_ys[index]\n",
    "    seq_len = [len(sentence)]\n",
    "    \n",
    "    sentence = Variable(LongTensor(sentence).view(1, -1), volatile=True)\n",
    "    \n",
    "    score, path = model.viterbi_decode(sentence, seq_len)\n",
    "    \n",
    "    print \"Sentence:\",ix_to_c(sentence.cpu().data[0], index2word)\n",
    "    print \"Ground true:\", ix_to_c(tag, index2tag)\n",
    "    print \"Prediction:\", ix_to_c(path, index2tag)\n",
    "    print \"Score: %.4f\"%(score)\n",
    "    \n",
    "def evaluate(model, val_Xs):\n",
    "    tags = []\n",
    "    for X, seq_len in get_order_sample(val_Xs):\n",
    "        score, path = model.viterbi_decode(X, seq_len)\n",
    "        tags.append(path)\n",
    "    return tags\n",
    "\n",
    "def generate_confusion_matrix(predicts, grounds, ix_to):\n",
    "    \"\"\"\n",
    "    predicts: list\n",
    "    grounds: list\n",
    "    labels: dict, index对应的tag\n",
    "    \"\"\"\n",
    "    #0,tp; 1, fn; 2, fp\n",
    "    confusion_matrix = np.zeros((len(ix_to)-2, 4))\n",
    "    for i,j in zip(grounds, predicts):\n",
    "        if i==j:\n",
    "            confusion_matrix[i,0] += 1\n",
    "        else:\n",
    "            confusion_matrix[i,1] += 1\n",
    "            confusion_matrix[j,2] += 1\n",
    "    ress = []\n",
    "    ress.append(['Label', 'Precision','Recall', 'F1', 'Support'])\n",
    "    ps = 0.0\n",
    "    rs = 0.0\n",
    "    f1s = 0.0\n",
    "    supports = 0\n",
    "    for i in xrange(confusion_matrix.shape[0]):\n",
    "        p = confusion_matrix[i][0]/(confusion_matrix[i][1]+confusion_matrix[i][0])\n",
    "        r = confusion_matrix[i][0]/(confusion_matrix[i][2]+confusion_matrix[i][0])\n",
    "        f1 = 2*p*r/(p+r)\n",
    "        ps += p\n",
    "        rs += r\n",
    "        f1s += f1\n",
    "        supports += confusion_matrix[i][0]\n",
    "        res = [ix_to[i], p, r, f1, confusion_matrix[i][0]]\n",
    "        ress.append(res)\n",
    "    ress.append(['Avg', ps/confusion_matrix.shape[0], rs/confusion_matrix.shape[0], \n",
    "                 f1s/confusion_matrix.shape[0], supports/confusion_matrix.shape[0]])\n",
    "    \n",
    "    for i in xrange(len(ress)):\n",
    "        for index, ele in enumerate(ress[i]):\n",
    "            if type(ele)==str:\n",
    "                print ele.ljust(10),\n",
    "            else:\n",
    "                s = '%.2f'%(ele)\n",
    "                print s.ljust(10),\n",
    "        print\n",
    "        if i == len(ress)-2:\n",
    "            print \n",
    "flatten = lambda x: [i for p in x for i in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def show_hot_map(predicts, grounds, to_ix):\n",
    "    confusion_matrix = np.zeros((len(to_ix)-2, len(to_ix)-2))\n",
    "    for i, j in zip(predicts, grounds):\n",
    "        confusion_matrix[i, j] += 1\n",
    "    confusion_matrix = confusion_matrix / np.sum(confusion_matrix, axis=1, keepdims=True)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    labels = []\n",
    "    for i in xrange(len(to_ix)-2):\n",
    "        labels.append(to_ix[i])\n",
    "    \n",
    "    cax = ax.matshow(confusion_matrix)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticklabels([''] + labels, rotation=90)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.set_title('Hot map')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = evaluate(model, val_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
