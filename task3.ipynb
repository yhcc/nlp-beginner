{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7f9cc020ba50>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH09JREFUeJzt3XuYXFWd7vHv29XdSSchISFNJuRCogSdhFHAiCij3PSA\n1zCOaPAWGBxGh0EZzzkaxjOPlzPxOI53HRwZucQbkEGU4IiKUVBBLuFOApGQBJIQSEJIwBCSTvfv\n/LFXh7Lo6qrqdPXurn4/z9NP771qV+3frkC9vdbetbYiAjMzs1o05V2AmZkNPQ4PMzOrmcPDzMxq\n5vAwM7OaOTzMzKxmDg8zM6uZw8OsF5JukPSBfnidFZJO6IeSzAYFh4cNOZLWSdol6Y+SnpB0maQx\nedfVm4iYExE3AEj6lKTv5VyS2X5xeNhQ9daIGAMcDcwF/k+tLyCpud+rGkIkFfKuwYYuh4cNaRGx\nEbgOOAJA0jhJF0vaJGmjpH/p/pCUdKakmyR9WdKTwKeK2r4haYekByWdXG5/kv5G0gOSnpL0c0mH\npvbXSNoqaVpaf3na5qVpfZ2k10s6Ffgn4F2p53SPpNMl3VGyn49KuqZMDRMkXSrpsbSPHxcd3+9K\ntg1Jh6XlyyR9U9JPJe0E/pekx4tDRNJfSbo3LTdJWijpYUlPSloiaUL1/zrWyBweNqSlD+s3AXel\npsuAvcBhwFHA/wCKz1m8ClgDTAIWFbU9DEwEPglc3dOHpKR5ZB/8bwfagd8ClwNExM3At4DFktqA\n7wH/HBEPFr9GRPwM+CxwZUSMiYiXA0uBmZL+vGjT9wHfKXPY3wVGAXOAg4Evl9muJ+9Ox30A8FVg\nJ3BSyeM/SMvnAacBxwOHAE8B/17DvqyBOTxsqPqxpO3A74Abgc9KmkQWJOdHxM6I2Ez2wTq/6HmP\nRcTXI2JvROxKbZuBr0RER0RcCawC3tzDPj8I/L+IeCAi9pKFwJHdvQ/gU8A44DZgI1V+0EbEbuBK\n4L0AkuYAM4CflG4raTLwRuCDEfFUqvnGavaTXBMRN0VEV0Q8RxZ+Z6TXPoDs/bu86Hg/EREbUo2f\nAt4x3If7LOPwsKHqtIg4MCIOjYi/T0FwKNACbJK0PYXLt8j+Ou+2vofX2hh/OkPoI2R/aZc6FPhq\n0WtvAwRMAYiIDrKezxHAF0tes5LFwLsliazXsSR9YJeaBmyLiKdqeO1ipcf/A+DtkkaQ9ajujIhH\n0mOHAj8qOt4HgE6yXpsNcw4PayTrgd3AxBQsB0bE2IiYU7RNTx/oU9KHdrfpwGNlXv/vil77wIho\nS0NWSJpCNux1KfDF9IHckxfUEBG3AHuA15INHX23l2OcIOnAHh7bSTacRarnzyrtOyJWkoXlG/nT\nIavufb2x5HhHpvNMNsw5PKxhRMQm4BdkH9xj0wnfF0s6vsJTDwY+LKlF0unAnwM/7WG7/wAuSMNK\n3SfnT0/LIut1XAycDWwC/m+Z/T0BzJBU+v/fd4BvAB0R8bsXPm3fMV4HXChpfKr5denhe4A5ko6U\nNJJsmKkaPwA+ArwO+K+S411UdFFAezrvY+bwsIbzfqAVWEl2gvcqYHKF59wKzAK2kp1MfkdEPFm6\nUUT8CPhX4ApJTwP3k/3FDvBhshD65zRcdRZwlqTX9rC/7g/oJyXdWdT+XbIhr0rfAXkf0AE8SHa+\n5vxU3x+AzwC/BB4iOx9UjcvJTor/KiK2FrV/lexk/i8kPQPcQnZxgRnyzaBsOJN0JvCBiPjLQVBL\nG1kYHB0RD+Vdj1lv3PMwGzw+BNzu4LChwJfcmQ0CktaRXbl1Ws6lmFXFw1ZmZlYzD1uZmVnNGnbY\nauLEiTFjxoy8yzAzG1LuuOOOrRHRXmm7hg2PGTNmsHz58rzLMDMbUiQ9UnkrD1uZmVkfODzMzKxm\nDg8zM6uZw8PMzGrm8DAzs5o5PMzMrGYODzMzq5nDo8RlN63l2nt6ug+QmZl1c3iU+N6tj3Ld/Zvy\nLsPMbFBzeJQQ4Lkizcx65/AoITk8zMwqcXiUECJwepiZ9cbhUcI9DzOzyhwePXB2mJn1zuFRQpJ7\nHmZmFTg8Sghw38PMrHcOjxI+52FmVpnDo4TkfoeZWSUOjxJChLseZma9cniUcM/DzKwyh0cJT09i\nZlaZw6OU5J6HmVkFDo8SWc/D8WFm1huHRwkp7wrMzAa/uoWHpEskbZZ0f1HbBEnXS3oo/R5f9NgF\nklZLWiXplKL2V0i6Lz32Nam+H+8+52FmVlk9ex6XAaeWtC0ElkXELGBZWkfSbGA+MCc950JJhfSc\nbwJ/C8xKP6Wv2a8kz6prZlZJ3cIjIn4DbCtpngcsTsuLgdOK2q+IiN0RsRZYDRwjaTIwNiJuiexE\nxHeKnlMX7nmYmVU20Oc8JkVE9z1eHwcmpeUpwPqi7TaktilpubS9R5LOkbRc0vItW7b0qUBPT2Jm\nVlluJ8xTT6JfP6Yj4qKImBsRc9vb2/v0Gr4ZlJlZZQMdHk+koSjS782pfSMwrWi7qaltY1ouba8f\n9zzMzCoa6PBYCixIywuAa4ra50saIWkm2Ynx29IQ19OSjk1XWb2/6Dl1ITw9iZlZJc31emFJlwMn\nABMlbQA+CXwOWCLpbOAR4J0AEbFC0hJgJbAXODciOtNL/T3ZlVttwHXpp24kiK567sHMbOirW3hE\nxBllHjq5zPaLgEU9tC8HjujH0nqVnfNwepiZ9cbfMC/hq63MzCpzeJTwlOxmZpU5PEr4ZlBmZpU5\nPEq452FmVpnDowfueJiZ9c7hUUK+GZSZWUUOjxICdz3MzCpweJTwOQ8zs8ocHiU8JbuZWWUOjxK+\nGZSZWWUOjxLueZiZVebwKOHpSczMKnN4vIAv1TUzq8ThUSLreTg+zMx64/AoobwLMDMbAhweZmZW\nM4dHCZ8wNzOrzOFRIruToNPDzKw3Do8S7nmYmVXm8Cjhua3MzCpzeJTwnQTNzCpzeJRyz8PMrCKH\nR4nsfh55V2FmNrg5PEr4ToJmZpU5PEpks+o6PszMeuPwKOGrrczMKnN4lPD9PMzMKsslPCT9o6QV\nku6XdLmkkZImSLpe0kPp9/ii7S+QtFrSKkmn1Lk2f8PczKyCAQ8PSVOADwNzI+IIoADMBxYCyyJi\nFrAsrSNpdnp8DnAqcKGkQt3qwz0PM7NK8hq2agbaJDUDo4DHgHnA4vT4YuC0tDwPuCIidkfEWmA1\ncEzdKvP0JGZmFQ14eETERuALwKPAJmBHRPwCmBQRm9JmjwOT0vIUYH3RS2xIbS8g6RxJyyUt37Jl\nS5/qk+/oYWZWUR7DVuPJehMzgUOA0ZLeW7xNZNfK1vz3f0RcFBFzI2Jue3t7H+vzpbpmZpXkMWz1\nemBtRGyJiA7gauA1wBOSJgOk35vT9huBaUXPn5ra6kL4Ul0zs0ryCI9HgWMljZIk4GTgAWApsCBt\nswC4Ji0vBeZLGiFpJjALuK1exXlKdjOzypoHeocRcaukq4A7gb3AXcBFwBhgiaSzgUeAd6btV0ha\nAqxM258bEZ31qs83gzIzq2zAwwMgIj4JfLKkeTdZL6Sn7RcBi+pdF7jnYWZWDX/DvISnJzEzq8zh\n8QJyz8PMrAKHRwkJtv5xd95lmJkNag6PEjev3grAtfc8lnMlZmaDl8OjxLonnwXgvo07cq7EzGzw\ncniUIc9SYmZWlsOjjILTw8ysLIdHGYUmh4eZWTkOjzLknoeZWVkOjzI8bGVmVp7DowyPWpmZlefw\nKKPJ6WFmVpbDo4wmD1uZmZXl8Cij4HfGzKwsf0SW4Z6HmVl5Do8yfKmumVl5VYWHpDsknStpfL0L\nGiwKzg4zs7Kq7Xm8CzgEuF3SFZJOkf80NzMbtqoKj4hYHRGfAA4HfgBcAjwi6dOSJtSzwIH28qnj\nAOj0DaHMzMqq+pyHpJcBXwT+DfghcDrwNPCr+pSWj28veCUA4dsJmpmV1VzNRpLuALYDFwMLI6L7\nVnu3SjquXsXlYVRrAYDOLoeHmVk5VYUHcHpErClukDQzItZGxNvrUFduumfT7XTPw8ysrGqHra6q\nsm3I6/5+h7PDzKy8Xnsekl4KzAHGSSruYYwFRtazsLx0T2n1bz9fxbknHpZvMWZmg1SlYauXAG8B\nDgTeWtT+DPC39SoqT74JlJlZZb2GR0RcA1wj6dUR8fsBqilX/vqKmVlllYatPhYRnwfeLemM0scj\n4sN92amkA4FvA0cAAfwNsAq4EpgBrAPeGRFPpe0vAM4GOoEPR8TP+7JfMzPrH5WGrR5Iv5f3836/\nCvwsIt4hqRUYBfwTsCwiPidpIbAQ+Lik2cB8snMvhwC/lHR4RHT2c01mZlalSsNW16bFKyPiueLH\nJE3syw4ljQNeB5yZ9rEH2CNpHnBC2mwxcAPwcWAecEX6bslaSauBY4BhMYxmZjYYVXup7m2Sju1e\nkfTXwM193OdMYAtwqaS7JH1b0mhgUkRsSts8DkxKy1OA9UXP35DazMwsJ9V+SfA9wCWSbiAbOjoI\nOGk/9nk0cF5E3Crpq2RDVPtEREiq+ZsWks4BzgGYPn16H8szM7NKqp0Y8T5gEfBB4ETgHyJiQx/3\nuQHYEBG3pvWryMLkCUmTAdLvzenxjcC0oudPTW091XlRRMyNiLnt7e19LM/MzCqp9n4eFwPnAy8D\nzgJ+IuncvuwwIh4H1kt6SWo6GVgJLAUWpLYFwDVpeSkwX9IISTOBWcBtfdm3mZn1j2qHre4DPhDZ\nVLNrJb0K+NJ+7Pc84PvpSqs1ZIHUBCyRdDbwCPBOgIhYIWkJWcDsBc6t95VWHzrhxVz827X13IWZ\n2ZBWVXhExFcktUmaHhGrImIH2fcu+iQi7gbm9vDQyWW2X0Q2bDYgmgRdntzKzKysaoet3grcDfws\nrR8paWk9C8tTk+RZdc3MelHtpbqfIvtuxXbY13N4UZ1qyl2TRIRvCGVmVk614dGRhqqKdfV3MYOF\np2U3M+tdteGxQtK7gYKkWZK+Tt+/JDjodU+s66ErM7OeVRse55HNLbUbuJzs3uXn16uovDWl9PBJ\nczOznlV7tdWzwCfST8PzsJWZWe8qTcl+LdmU6T2KiLf1e0WDwL5hqy6nh5lZTyr1PL4wIFUMMgUP\nW5mZ9arSlOw3di+nb4O/lKwnsipNpd6Quu8m6I6HmVnPqjrnIenNwH8ADwMCZkr6u4i4rp7F5aV7\n2KrL6WFm1qNq57b6InBiRKwGkPRi4L+BBg0PD1uZmfWm2kt1n+kOjmQN8Ewd6hkUnr9UN+dCzMwG\nqWp7Hssl/RRYQnbO43TgdklvB4iIq+tUXy66h608PYmZWc+qDY+RwBPA8Wl9C9AGvJUsTBosPLL0\n8DfMzcx6VjE8JBWAeyPiywNQz6BQ8NVWZma9qnjOI9146YwBqGXQkK+2MjPrVbXDVjdJ+gZwJbCz\nuzEi7qxLVTnz1VZmZr2rNjyOTL8/U9QWwEn9W87gUPDVVmZmvap2YsQT613IYLJv2Mo9DzOzHlV7\nG9pJki6WdF1any2pz/cwH+z2DVu562Fm1qNqvyR4GfBz4JC0/gca+H4eHrYyM+tdteExMSKWkG49\nGxF7gc66VZWzJg9bmZn1qtrw2CnpINK9PSQdC5Te07xh7PuSoLseZmY9qvZqq48CS4EXSboJaAfe\nUbeqcjaypQDA7r0N27kyM9sv1YbHSuBHwLNkEyL+mOy8R0Nqa83CY9eerpwrMTMbnKodtvoO2Y2g\nPgt8HTgc+G69ispbW+p57Opwz8PMrCfV9jyOiIjZReu/lrSyHgUNBiNbskx1eJiZ9azansed6SQ5\nAJJeBSzfnx1LKki6S9JP0voESddLeij9Hl+07QWSVktaJemU/dlvNbrPeTy3x+FhZtaTasPjFcDN\nktZJWgf8HnilpPsk3dvHfX8EeKBofSGwLCJmAcvSOpJmA/OBOcCpwIVppt+68bCVmVnvqh22OrU/\ndyppKvBmYBHZlVwA84AT0vJi4Abg46n9iojYDayVtBo4hizA6mLfCXOHh5lZj6qd2+qRft7vV4CP\nAQcUtU2KiE1p+XFgUlqeAtxStN2G1PYCks4BzgGYPn16n4sb2Vyg0CSe3tXR59cwM2tk1Q5b9RtJ\nbwE2R8Qd5baJ7P6vNX9DLyIuioi5ETG3vb29zzU2NYnJ40aycfuuPr+GmVkjq3bYqj8dB7xN0pvI\nbm87VtL3gCckTY6ITZImA5vT9huBaUXPn5ra6mra+FGs3/ZsvXdjZjYkDXjPIyIuiIipETGD7ET4\nryLivWTfYF+QNlsAXJOWlwLzJY2QNBOYBdxW7zpHj2hmV4e/JGhm1pM8eh7lfA5YkqZ6fwR4J0BE\nrJC0hOxb7nuBc9OtceuqpSA6uxweZmY9yTU8IuIGsquqiIgngZPLbLeI7MqsAVNoEns7PTGimVlP\nBnzYaqhobhJ7PauumVmPHB5lNBeaPCW7mVkZDo8ymptER6fPeZiZ9cThUUahSe55mJmV4fAoo6XQ\n5HMeZmZlODzKyK628rCVmVlPHB5l+GorM7PyHB5lNBd8zsPMrByHRxmFpuycRzZHo5mZFXN4lNHc\nJAD3PszMeuDwKKO5kIWHz3uYmb2Qw6OM7p6Hw8PM7IUcHmU0N2VvTacnRzQzewGHRxnPD1v5ux5m\nZqUcHmUUPGxlZlaWw6OMtpYCALv21P2+U2ZmQ47Do4xRrdl9snbu2ZtzJWZmg4/Do4wxI7Lw+K/l\nG3KuxMxs8HF4lDFqRDZsddnN6/wtczOzEg6PMka3Pn979+c6fMWVmVkxh0cZhaJ3ZleHT5qbmRVz\neJQxdfyofcsODzOzP+XwKGNkS4GvnXEUALt8xZWZ2Z9wePTi+e96+JyHmVkxh0cv9oWHh63MzP6E\nw6MXba0ODzOznjg8enHAyOxy3TVb/phzJWZmg8uAh4ekaZJ+LWmlpBWSPpLaJ0i6XtJD6ff4oudc\nIGm1pFWSThmoWg9rH8OksSO469HtA7VLM7MhIY+ex17gf0bEbOBY4FxJs4GFwLKImAUsS+ukx+YD\nc4BTgQslFQai0KYmcdDoEezc7autzMyKDXh4RMSmiLgzLT8DPABMAeYBi9Nmi4HT0vI84IqI2B0R\na4HVwDEDVe+YEc0885zDw8ysWK7nPCTNAI4CbgUmRcSm9NDjwKS0PAVYX/S0Damtp9c7R9JyScu3\nbNnSLzXes2E7t63bxr0bPHRlZtYtt/CQNAb4IXB+RDxd/FhkMxHWPBthRFwUEXMjYm57e3u/1Ll7\nb/Ydj7vXOzzMzLrlEh6SWsiC4/sRcXVqfkLS5PT4ZGBzat8ITCt6+tTUNqDGtbUM9C7NzAatPK62\nEnAx8EBEfKnooaXAgrS8ALimqH2+pBGSZgKzgNsGqt5XHDq+8kZmZsNMHj2P44D3ASdJujv9vAn4\nHPAGSQ8Br0/rRMQKYAmwEvgZcG5EDNi39r6e5rd61rejNTPbp7nyJv0rIn4HqMzDJ5d5ziJgUd2K\n6sWoVt/L3MyslL9hXoGnKDEzeyGHRwWthSaaBOu3PZt3KWZmg4bDowJJHHzASG56eGvepZiZDRoO\njyq8/egpPLb9OfZ2+r4eZmbg8KjKlPFtdHYFm3Y8l3cpZmaDgsOjCi+bciAAt6/blnMlZmaDg8Oj\nCnMOGcvEMa3c+If+mS/LzGyoc3hUoalJzDlkHGu37sy7FDOzQcHhUaVxbS3s2NWRdxlmZoOCw6NK\n49paeNrhYWYGODyqNratmaef20s2W7yZ2fDm8KjSuLYWOruCDU/tyrsUM7PcOTyqdPzhBwNwg6+4\nMjNzeFTrsIPH0NwkNm13z8PMzOFRpUKTmDR2pIetzMxweNTk6EPHs/Sex/j1qs2VNzYza2AOjxoc\nPT2bpuSsS2/PuRIzs3w5PGow78gpAMw4aFTOlZiZ5cvhUYMJo1t5819M5tk9nXR1+fseZjZ8OTxq\ndMSUcWx+Zjf/+ds1eZdiZpYbh0eNzjpuBgAX/WYNz/m+5mY2TDk8ajSypcAxMybw5M49fPraFXmX\nY2aWC4dHH+xJt6P98V2P5VyJmVk+HB59cO6JhwGwq6OTLc/szrkaM7OB5/DogzfMnsQ15x4HwOu/\ndKPPfZjZsOPw6KOXTzuQE1/Szo5dHZz8xRvp9KW7ZjaMODz2w4XveQXHH97Oxu27WHDJbWx/dk/e\nJZmZDYghEx6STpW0StJqSQvzrgegrbXApWe+kjNfM4Pfrd7Kqz67jM9cu5K7Hn3KPREza2gaCnfG\nk1QA/gC8AdgA3A6cEREryz1n7ty5sXz58gGqEH7/8JN86zcPc/PqJ9nT2cW4thaOmDKWWQcfwKSx\nIzlodCvjR7cyYXQL49paaC0UaGkWLYWm9JMtNzcJSQNW93D2y5VPUCiI42e109Tk99yGrpsf3koE\nvHLGBFqb969PIOmOiJhbabvm/drLwDkGWB0RawAkXQHMA8qGx0B79YsP4tUvPogduzr49YObuWXN\nk6x47GmWLF/Ps3tqO6HeWmiiuaB9oSKJ7o82CbrXsuXu9ufbin93E9q3ffHrDVcdXV2s35ZNrz+y\npYmJY0ZQaBIFpffJAW5DRFcEa7bsBGDMiGYOHjuC/z7vtbS1Fuq636ESHlOA9UXrG4BXlW4k6Rzg\nHIDp06cPTGUlxrW1cNpRUzjtqGwSxYhgV0cnT/5xD089u4dtO/ewY1cHHZ1BR2cXezu72JOWO/Z2\n0dFVtNyZrXf3DiOyH4AgipbTYxQ1FIlUx/PbZevD/QPyrS87hMMnHcB9G3ewbeceuiLoCjxvmdVN\nENTjT7dXv+ggjpk5geXrnmLbs3v2u/dRjaESHlWJiIuAiyAbtsq5HCD7C3ZUazOjJjQzbYJn4x2M\nuoPebKjrnvl7IAyVE+YbgWlF61NTm5mZ5WCohMftwCxJMyW1AvOBpTnXZGY2bA2JYauI2CvpH4Cf\nAwXgkojwrIRmZjkZEuEBEBE/BX6adx1mZjZ0hq3MzGwQcXiYmVnNHB5mZlYzh4eZmdVsSMxt1ReS\ntgCP9PHpE4Gt/VjOUOBjHh58zMPD/hzzoRHRXmmjhg2P/SFpeTUTgzUSH/Pw4GMeHgbimD1sZWZm\nNXN4mJlZzRwePbso7wJy4GMeHnzMw0Pdj9nnPMzMrGbueZiZWc0cHmZmVjOHRxFJp0paJWm1pIV5\n19NfJE2T9GtJKyWtkPSR1D5B0vWSHkq/xxc954L0PqySdEp+1e8fSQVJd0n6SVpv6GOWdKCkqyQ9\nKOkBSa8eBsf8j+m/6/slXS5pZKMds6RLJG2WdH9RW83HKOkVku5Lj31N+3M70YjwT3bepwA8DLwI\naAXuAWbnXVc/Hdtk4Oi0fADwB2A28HlgYWpfCPxrWp6djn8EMDO9L4W8j6OPx/5R4AfAT9J6Qx8z\nsBj4QFpuBQ5s5GMmu0X1WqAtrS8Bzmy0YwZeBxwN3F/UVvMxArcBxwICrgPe2Nea3PN43jHA6ohY\nExF7gCuAeTnX1C8iYlNE3JmWnwEeIPufbh7Zhw3p92lpeR5wRUTsjoi1wGqy92dIkTQVeDPw7aLm\nhj1mSePIPmQuBoiIPRGxnQY+5qQZaJPUDIwCHqPBjjkifgNsK2mu6RglTQbGRsQtkSXJd4qeUzOH\nx/OmAOuL1jektoYiaQZwFHArMCkiNqWHHgcmpeVGeS++AnwM6Cpqa+RjnglsAS5NQ3XfljSaBj7m\niNgIfAF4FNgE7IiIX9DAx1yk1mOckpZL2/vE4TGMSBoD/BA4PyKeLn4s/SXSMNdtS3oLsDki7ii3\nTaMdM9lf4EcD34yIo4CdZMMZ+zTaMadx/nlkwXkIMFrSe4u3abRj7kkex+jweN5GYFrR+tTU1hAk\ntZAFx/cj4urU/ETqypJ+b07tjfBeHAe8TdI6siHIkyR9j8Y+5g3Ahoi4Na1fRRYmjXzMrwfWRsSW\niOgArgZeQ2Mfc7daj3FjWi5t7xOHx/NuB2ZJmimpFZgPLM25pn6Rrqi4GHggIr5U9NBSYEFaXgBc\nU9Q+X9IISTOBWWQn2oaMiLggIqZGxAyyf8tfRcR7aexjfhxYL+klqelkYCUNfMxkw1XHShqV/js/\nmeycXiMfc7eajjENcT0t6dj0Xr2/6Dm1y/sqgsH0A7yJ7Eqkh4FP5F1PPx7XX5J1ae8F7k4/bwIO\nApYBDwG/BCYUPecT6X1YxX5ckTEYfoATeP5qq4Y+ZuBIYHn6t/4xMH4YHPOngQeB+4Hvkl1l1FDH\nDFxOdk6ng6yHeXZfjhGYm96nh4FvkGYZ6cuPpycxM7OaedjKzMxq5vAwM7OaOTzMzKxmDg8zM6uZ\nw8PMzGrm8DDrI0mdku4u+um3mZglzSieQdVssGnOuwCzIWxXRByZdxFmeXDPw6yfSVon6fPpvgm3\nSTostc+Q9CtJ90paJml6ap8k6UeS7kk/r0kvVZD0n+leFb+Q1JbbQZmVcHiY9V1bybDVu4oe2xER\nf0H2Ld6vpLavA4sj4mXA94GvpfavATdGxMvJ5qJakdpnAf8eEXOA7cBf1/l4zKrmb5ib9ZGkP0bE\nmB7a1wEnRcSaNCHl4xFxkKStwOSI6EjtmyJioqQtwNSI2F30GjOA6yNiVlr/ONASEf9S/yMzq8w9\nD7P6iDLLtdhdtNyJz1HaIOLwMKuPdxX9/n1avplshl+A9wC/TcvLgA/BvnuujxuoIs36yn/JmPVd\nm6S7i9Z/FhHdl+uOl3QvWe/hjNR2Htld/v432R3/zkrtHwEuknQ2WQ/jQ2QzqJoNWj7nYdbP0jmP\nuRGxNe9azOrFw1ZmZlYz9zzMzKxm7nmYmVnNHB5mZlYzh4eZmdXM4WFmZjVzeJiZWc3+PwnUjpUU\nQKycAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9cc0279810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(perplexities)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('perplexity')\n",
    "plt.title('Perplexity curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "偃露吹苍马路扫，天尘今太红。\n",
      "\n",
      "要自花夫亦将，荒与川浸溜。是节东篱菊，纷披为谁秀。自壮几时开，新声子夜歌。\n",
      "\n",
      "衢所天上今上在，独丸日月十经秋。秋来未曾馀力，君人清再欢间。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#测试诗句生成\n",
    "for i in xrange(3):\n",
    "    start_word = lang.get_word(random.randint(0, len(lang.word2index)))\n",
    "    randomDemo(model, lang, start_word)\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于训练样本也比较小，所以生成的句子效果不算太好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = open('poemFromTang.txt', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {'<pad>':0, '<s>':1, '</s>':2, '<unknown>':3}\n",
    "        self.index2word = {v:k for k,v in self.word2index.iteritems()}\n",
    "        self.n_words = len(self.word2index)\n",
    "    def addChineseSentence(self, sentence):\n",
    "        sentence = sentence.strip()\n",
    "        for word in sentence:\n",
    "            self.addChineseWord(word)\n",
    "            \n",
    "    def addChineseWord(self, word):\n",
    "        if not self.word2index.has_key(word):\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "    \n",
    "    def vectorize(self, sentence):\n",
    "        sentence = sentence.strip()\n",
    "        nums = []\n",
    "        for char in sentence:\n",
    "            if self.word2index.has_key(char):\n",
    "                nums.append(self.word2index[char])\n",
    "            else:\n",
    "                nums.append(self.word2index['<unknown>'])\n",
    "        return nums\n",
    "    def get_index(self, word):\n",
    "        if self.word2index.has_key(word):\n",
    "            return self.word2index[word]\n",
    "        else:\n",
    "            return self.word2index['<unknown>']\n",
    "    def get_word(self, index):\n",
    "        return self.index2word[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poems = [] #每一个元素是一首诗词\n",
    "poem = ''\n",
    "for line in lines:\n",
    "    if line=='\\n':\n",
    "        if len(poem)>0:\n",
    "            poems.append(poem)\n",
    "        poem = ''\n",
    "    else:\n",
    "        poem += line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({16: 1,\n",
       "         24: 2,\n",
       "         32: 22,\n",
       "         33: 1,\n",
       "         48: 69,\n",
       "         56: 2,\n",
       "         58: 1,\n",
       "         59: 1,\n",
       "         64: 13,\n",
       "         70: 1,\n",
       "         72: 2,\n",
       "         74: 1,\n",
       "         80: 3,\n",
       "         81: 1,\n",
       "         83: 1,\n",
       "         88: 1,\n",
       "         92: 1,\n",
       "         93: 1,\n",
       "         96: 1,\n",
       "         101: 1,\n",
       "         104: 1,\n",
       "         105: 1,\n",
       "         120: 1,\n",
       "         128: 2,\n",
       "         129: 1,\n",
       "         132: 3,\n",
       "         135: 1,\n",
       "         144: 2,\n",
       "         148: 1,\n",
       "         152: 1,\n",
       "         156: 1,\n",
       "         160: 2,\n",
       "         161: 1,\n",
       "         166: 1,\n",
       "         168: 1,\n",
       "         192: 1,\n",
       "         204: 1,\n",
       "         216: 1,\n",
       "         224: 1,\n",
       "         244: 1,\n",
       "         258: 1,\n",
       "         282: 1,\n",
       "         288: 2,\n",
       "         312: 1,\n",
       "         320: 2,\n",
       "         336: 1,\n",
       "         407: 1,\n",
       "         600: 1,\n",
       "         892: 1,\n",
       "         1107: 1})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(map(len, poems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有一些诗比较长。本来可以直接丢弃这部分数据，但考虑到本来数据已经不太足够，这里讲数字超过的诗拆分为长度小于74的集合。拆分的原则是，使得长于74的诗，分为多部分，每部分长度接近48。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_poems = []\n",
    "\n",
    "for poem in poems:\n",
    "    if len(poem)<74:\n",
    "        new_poems.append(poem)\n",
    "    else:\n",
    "        phrases = poem.split('。'.decode('utf-8'))\n",
    "        poem = ''\n",
    "        for ph in phrases:\n",
    "            if len(poem) + len(ph)>48:\n",
    "                new_poems.append(poem.strip())\n",
    "                poem = ph + '。'.decode('utf-8')\n",
    "            else:\n",
    "                poem += ph + '。'.decode('utf-8')\n",
    "        if len(poem)>=16:\n",
    "            new_poems.append(poem.strip())\n",
    "            \n",
    "train_Xs = []\n",
    "\n",
    "for poem in new_poems:\n",
    "    nums = lang.vectorize(poem)\n",
    "    train_Xs.append(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch as T\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "USE_CUDA = T.cuda.is_available()\n",
    "\n",
    "FloatTensor = T.cuda.FloatTensor if USE_CUDA else T.FloatTensor\n",
    "LongTensor = T.cuda.LongTensor if USE_CUDA else T.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PoemLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers):\n",
    "        super(PoemLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform(self.embedding.state_dict()['weight'])\n",
    "        nn.init.xavier_uniform(self.fc.state_dict()['weight'])\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = Variable(T.zeros(self.n_layers,batch_size, self.hidden_size).type(FloatTensor))\n",
    "        c = Variable(T.zeros(self.n_layers, batch_size, self.hidden_size).type(FloatTensor))\n",
    "        hiddens = (hidden, c)\n",
    "        return hiddens\n",
    "        \n",
    "    def forward(self, x, seq_len, hiddens):\n",
    "        \"\"\"\n",
    "        x: B x L\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        length = x.size(1)\n",
    "        x = self.embedding(x).view(batch_size, -1, self.embedding_dim)  #1 x L x D\n",
    "        pack = nn.utils.rnn.pack_padded_sequence(x, seq_len, batch_first=True)\n",
    "        output, hiddens = self.lstm(pack, hiddens)\n",
    "        output,  _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        output = self.fc(output.contiguous().view(-1, self.hidden_size))\n",
    "        return F.log_softmax(output), hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(Xs, batch_size=32):\n",
    "    Xs = np.copy(Xs)\n",
    "    random.shuffle(Xs)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex<len(Xs):\n",
    "        batch = Xs[sindex:eindex]\n",
    "        batch = [[1]+b+[2] for b in batch]\n",
    "        batch = sorted(batch, key=lambda x:len(x), reverse=True)\n",
    "        max_len = len(batch[0])\n",
    "        X = []\n",
    "        y = []\n",
    "        seq_len = []\n",
    "        for b in batch:\n",
    "            seq_len.append(len(b))\n",
    "            while len(b)<max_len:\n",
    "                b.append(0)\n",
    "            X.append(b[:-1])\n",
    "            y.append(b[1:])\n",
    "        yield X, y, seq_len\n",
    "        sindex = eindex\n",
    "        eindex += batch_size\n",
    "    if sindex<len(Xs):\n",
    "        batch = Xs[sindex:]\n",
    "        batch = [[1]+b+[2] for b in batch]\n",
    "        batch = sorted(batch, key=lambda x:len(x), reverse=True)\n",
    "        max_len = len(batch[0])\n",
    "        X = []\n",
    "        y = []\n",
    "        seq_len = []\n",
    "        for b in batch:\n",
    "            seq_len.append(len(b))\n",
    "            while len(b)<max_len:\n",
    "                b.append(0)\n",
    "            X.append(b[:-1])\n",
    "            y.append(b[1:])\n",
    "        yield X, y, seq_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 154, 102, 183, 184, 185, 61, 186, 10, 187, 188, 189, 190, 191, 192, 117, 18, 91, 102, 193, 194, 195, 69, 196, 10, 71, 189, 197, 61, 198, 9, 169, 18, 199, 200, 201, 202, 203, 127, 204, 10, 205, 185, 107, 206, 56, 207, 208, 18], [1, 255, 122, 256, 10, 71, 189, 257, 10, 258, 259, 137, 260, 261, 262, 214, 10, 216, 263, 264, 133, 63, 114, 265, 18, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] [[154, 102, 183, 184, 185, 61, 186, 10, 187, 188, 189, 190, 191, 192, 117, 18, 91, 102, 193, 194, 195, 69, 196, 10, 71, 189, 197, 61, 198, 9, 169, 18, 199, 200, 201, 202, 203, 127, 204, 10, 205, 185, 107, 206, 56, 207, 208, 18, 2], [255, 122, 256, 10, 71, 189, 257, 10, 258, 259, 137, 260, 261, 262, 214, 10, 216, 263, 264, 133, 63, 114, 265, 18, 18, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] [50, 27]\n",
      "<s> 人 生 得 意 须 尽 欢 ， 莫 使 金 尊 空 对 月 。 天 生 我 材 必 有 用 ， 千 金 散 尽 还 复 来 。 烹 羊 宰 牛 且 为 乐 ， 会 须 一 饮 三 百 杯 。\n"
     ]
    }
   ],
   "source": [
    "for Xs, ys, seq_len in get_batch(train_Xs[:10], 2):\n",
    "    print Xs, ys, seq_len\n",
    "    for i in Xs[0]:\n",
    "        print lang.get_word(i),\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainIter(model, Xs, optimizer, criterion, scheduler=None, num_epochs=20, print_every=50, batch_size=32):\n",
    "    perplexities = []\n",
    "    for epoch in xrange(num_epochs):\n",
    "        print '-' * 10\n",
    "        epoch_loss = 0.0\n",
    "        counter = 0\n",
    "        x_counter = 0\n",
    "        perplex = 0.0\n",
    "        for X, y, seq_len in get_batch(Xs, batch_size):\n",
    "            perplexity = 0.0\n",
    "            X = Variable(LongTensor(X))\n",
    "            y = Variable(LongTensor(y).view(-1))\n",
    "\n",
    "            hiddens = model.init_hidden(X.size(0))\n",
    "\n",
    "            loss = 0.0\n",
    "            \n",
    "            output, _ = model(X, seq_len, hiddens)\n",
    "            loss = criterion(output, y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.data[0]\n",
    "            counter += 1\n",
    "            x_counter += sum(seq_len) \n",
    "            perplex += loss.data[0]*y.size(0)\n",
    "        if scheduler!=None:\n",
    "            scheduler.step()\n",
    "\n",
    "        perplexities.append(np.power(2, perplex / x_counter))\n",
    "        print '[{}/{}] loss:{:.4f} perplex:{:.2f}'.format(epoch + 1, num_epochs, epoch_loss / counter,\n",
    "                                                          perplexities[-1])\n",
    "    return perplexities\n",
    "\n",
    "\n",
    "def randomDemo(model, lang, start_word='<s>'):\n",
    "    input = Variable(LongTensor([lang.get_index(start_word)]), volatile=True)\n",
    "    words = []\n",
    "    if start_word!='<s>':\n",
    "        words.append(start_word)\n",
    "    hiddens = model.init_hidden(1)\n",
    "    idx = 0\n",
    "    while idx<48:\n",
    "        output, hiddens = model(input.view(1,1), [1],hiddens)\n",
    "        predict = output.data.cpu().max(1)[1][0]\n",
    "        if predict==2:\n",
    "            break\n",
    "        words.append(lang.get_word(predict))\n",
    "        input = Variable(LongTensor([predict]), volatile=True)\n",
    "        idx += 1\n",
    "        \n",
    "    print \"\".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_size = 50\n",
    "hidden_size = 128\n",
    "\n",
    "model = PoemLSTM(lang.n_words, embed_size, hidden_size, 2)\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = StepLR(optimizer, step_size=200, gamma=0.9)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "[1/1000] loss:7.3204 perplex:1104.54\n",
      "----------\n",
      "[2/1000] loss:6.8972 perplex:662.04\n",
      "----------\n",
      "[3/1000] loss:6.8203 perplex:745.82\n",
      "----------\n",
      "[4/1000] loss:6.7315 perplex:585.34\n",
      "----------\n",
      "[5/1000] loss:6.6463 perplex:632.48\n",
      "----------\n",
      "[6/1000] loss:6.4941 perplex:444.19\n",
      "----------\n",
      "[7/1000] loss:6.3972 perplex:400.29\n",
      "----------\n",
      "[8/1000] loss:6.3028 perplex:387.33\n",
      "----------\n",
      "[9/1000] loss:6.1898 perplex:286.76\n",
      "----------\n",
      "[10/1000] loss:6.1070 perplex:250.95\n",
      "----------\n",
      "[11/1000] loss:6.0011 perplex:268.25\n",
      "----------\n",
      "[12/1000] loss:5.8969 perplex:262.90\n",
      "----------\n",
      "[13/1000] loss:5.7745 perplex:220.46\n",
      "----------\n",
      "[14/1000] loss:5.6732 perplex:217.78\n",
      "----------\n",
      "[15/1000] loss:5.5741 perplex:182.80\n",
      "----------\n",
      "[16/1000] loss:5.4801 perplex:198.00\n",
      "----------\n",
      "[17/1000] loss:5.3805 perplex:155.82\n",
      "----------\n",
      "[18/1000] loss:5.2965 perplex:155.02\n",
      "----------\n",
      "[19/1000] loss:5.2041 perplex:140.98\n",
      "----------\n",
      "[20/1000] loss:5.1178 perplex:133.58\n",
      "----------\n",
      "[21/1000] loss:5.0310 perplex:128.92\n",
      "----------\n",
      "[22/1000] loss:4.9482 perplex:102.31\n",
      "----------\n",
      "[23/1000] loss:4.8506 perplex:100.02\n",
      "----------\n",
      "[24/1000] loss:4.7664 perplex:85.15\n",
      "----------\n",
      "[25/1000] loss:4.6700 perplex:80.12\n",
      "----------\n",
      "[26/1000] loss:4.5855 perplex:72.89\n",
      "----------\n",
      "[27/1000] loss:4.4845 perplex:78.79\n",
      "----------\n",
      "[28/1000] loss:4.4117 perplex:61.81\n",
      "----------\n",
      "[29/1000] loss:4.3288 perplex:57.98\n",
      "----------\n",
      "[30/1000] loss:4.2261 perplex:58.61\n",
      "----------\n",
      "[31/1000] loss:4.1504 perplex:50.39\n",
      "----------\n",
      "[32/1000] loss:4.1539 perplex:38.25\n",
      "----------\n",
      "[33/1000] loss:3.9798 perplex:48.64\n",
      "----------\n",
      "[34/1000] loss:3.9250 perplex:38.87\n",
      "----------\n",
      "[35/1000] loss:3.8012 perplex:37.75\n",
      "----------\n",
      "[36/1000] loss:3.7551 perplex:30.83\n",
      "----------\n",
      "[37/1000] loss:3.6628 perplex:29.66\n",
      "----------\n",
      "[38/1000] loss:3.6297 perplex:25.46\n",
      "----------\n",
      "[39/1000] loss:3.4833 perplex:27.12\n",
      "----------\n",
      "[40/1000] loss:3.4298 perplex:24.20\n",
      "----------\n",
      "[41/1000] loss:3.2922 perplex:24.11\n",
      "----------\n",
      "[42/1000] loss:3.2601 perplex:20.05\n",
      "----------\n",
      "[43/1000] loss:3.1274 perplex:20.22\n",
      "----------\n",
      "[44/1000] loss:3.0492 perplex:18.20\n",
      "----------\n",
      "[45/1000] loss:3.0282 perplex:15.64\n",
      "----------\n",
      "[46/1000] loss:2.9037 perplex:15.35\n",
      "----------\n",
      "[47/1000] loss:2.8798 perplex:13.52\n",
      "----------\n",
      "[48/1000] loss:2.7260 perplex:13.72\n",
      "----------\n",
      "[49/1000] loss:2.6815 perplex:12.19\n",
      "----------\n",
      "[50/1000] loss:2.5896 perplex:11.85\n",
      "----------\n",
      "[51/1000] loss:2.5368 perplex:11.03\n",
      "----------\n",
      "[52/1000] loss:2.4567 perplex:10.60\n",
      "----------\n",
      "[53/1000] loss:2.4234 perplex:9.78\n",
      "----------\n",
      "[54/1000] loss:2.3161 perplex:9.41\n",
      "----------\n",
      "[55/1000] loss:2.2594 perplex:8.74\n",
      "----------\n",
      "[56/1000] loss:2.1890 perplex:8.14\n",
      "----------\n",
      "[57/1000] loss:2.1637 perplex:7.57\n",
      "----------\n",
      "[58/1000] loss:2.1222 perplex:7.14\n",
      "----------\n",
      "[59/1000] loss:1.9895 perplex:6.96\n",
      "----------\n",
      "[60/1000] loss:1.9898 perplex:6.45\n",
      "----------\n",
      "[61/1000] loss:1.8996 perplex:6.22\n",
      "----------\n",
      "[62/1000] loss:1.8465 perplex:6.00\n",
      "----------\n",
      "[63/1000] loss:1.8335 perplex:5.61\n",
      "----------\n",
      "[64/1000] loss:1.8087 perplex:5.34\n",
      "----------\n",
      "[65/1000] loss:1.7339 perplex:5.18\n",
      "----------\n",
      "[66/1000] loss:1.7161 perplex:4.92\n",
      "----------\n",
      "[67/1000] loss:1.7196 perplex:4.74\n",
      "----------\n",
      "[68/1000] loss:1.6444 perplex:4.64\n",
      "----------\n",
      "[69/1000] loss:1.6511 perplex:4.41\n",
      "----------\n",
      "[70/1000] loss:1.6197 perplex:4.36\n",
      "----------\n",
      "[71/1000] loss:1.5898 perplex:4.22\n",
      "----------\n",
      "[72/1000] loss:1.4785 perplex:4.11\n",
      "----------\n",
      "[73/1000] loss:1.4253 perplex:3.92\n",
      "----------\n",
      "[74/1000] loss:1.3931 perplex:3.74\n",
      "----------\n",
      "[75/1000] loss:1.4278 perplex:3.57\n",
      "----------\n",
      "[76/1000] loss:1.4062 perplex:3.48\n",
      "----------\n",
      "[77/1000] loss:1.3147 perplex:3.41\n",
      "----------\n",
      "[78/1000] loss:1.2737 perplex:3.28\n",
      "----------\n",
      "[79/1000] loss:1.1816 perplex:3.16\n",
      "----------\n",
      "[80/1000] loss:1.1937 perplex:3.00\n",
      "----------\n",
      "[81/1000] loss:1.1640 perplex:2.90\n",
      "----------\n",
      "[82/1000] loss:1.0847 perplex:2.84\n",
      "----------\n",
      "[83/1000] loss:1.0518 perplex:2.77\n",
      "----------\n",
      "[84/1000] loss:1.0178 perplex:2.69\n",
      "----------\n",
      "[85/1000] loss:1.0085 perplex:2.60\n",
      "----------\n",
      "[86/1000] loss:0.9630 perplex:2.54\n",
      "----------\n",
      "[87/1000] loss:0.9641 perplex:2.46\n",
      "----------\n",
      "[88/1000] loss:0.9692 perplex:2.40\n",
      "----------\n",
      "[89/1000] loss:0.9309 perplex:2.39\n",
      "----------\n",
      "[90/1000] loss:0.9031 perplex:2.37\n",
      "----------\n",
      "[91/1000] loss:0.9153 perplex:2.28\n",
      "----------\n",
      "[92/1000] loss:0.8760 perplex:2.26\n",
      "----------\n",
      "[93/1000] loss:0.8513 perplex:2.22\n",
      "----------\n",
      "[94/1000] loss:0.8118 perplex:2.17\n",
      "----------\n",
      "[95/1000] loss:0.8038 perplex:2.11\n",
      "----------\n",
      "[96/1000] loss:0.8022 perplex:2.06\n",
      "----------\n",
      "[97/1000] loss:0.7371 perplex:2.05\n",
      "----------\n",
      "[98/1000] loss:0.7703 perplex:1.98\n",
      "----------\n",
      "[99/1000] loss:0.7239 perplex:1.95\n",
      "----------\n",
      "[100/1000] loss:0.6898 perplex:1.93\n",
      "----------\n",
      "[101/1000] loss:0.7103 perplex:1.88\n",
      "----------\n",
      "[102/1000] loss:0.6747 perplex:1.86\n",
      "----------\n",
      "[103/1000] loss:0.6385 perplex:1.82\n",
      "----------\n",
      "[104/1000] loss:0.6180 perplex:1.77\n",
      "----------\n",
      "[105/1000] loss:0.6094 perplex:1.74\n",
      "----------\n",
      "[106/1000] loss:0.5831 perplex:1.71\n",
      "----------\n",
      "[107/1000] loss:0.5486 perplex:1.68\n",
      "----------\n",
      "[108/1000] loss:0.5235 perplex:1.65\n",
      "----------\n",
      "[109/1000] loss:0.5374 perplex:1.62\n",
      "----------\n",
      "[110/1000] loss:0.5156 perplex:1.61\n",
      "----------\n",
      "[111/1000] loss:0.5019 perplex:1.60\n",
      "----------\n",
      "[112/1000] loss:0.4845 perplex:1.57\n",
      "----------\n",
      "[113/1000] loss:0.4681 perplex:1.55\n",
      "----------\n",
      "[114/1000] loss:0.4388 perplex:1.53\n",
      "----------\n",
      "[115/1000] loss:0.4471 perplex:1.51\n",
      "----------\n",
      "[116/1000] loss:0.4298 perplex:1.50\n",
      "----------\n",
      "[117/1000] loss:0.4350 perplex:1.48\n",
      "----------\n",
      "[118/1000] loss:0.4034 perplex:1.47\n",
      "----------\n",
      "[119/1000] loss:0.4014 perplex:1.45\n",
      "----------\n",
      "[120/1000] loss:0.4025 perplex:1.44\n",
      "----------\n",
      "[121/1000] loss:0.3847 perplex:1.43\n",
      "----------\n",
      "[122/1000] loss:0.3760 perplex:1.41\n",
      "----------\n",
      "[123/1000] loss:0.3634 perplex:1.41\n",
      "----------\n",
      "[124/1000] loss:0.3493 perplex:1.39\n",
      "----------\n",
      "[125/1000] loss:0.3357 perplex:1.38\n",
      "----------\n",
      "[126/1000] loss:0.3278 perplex:1.36\n",
      "----------\n",
      "[127/1000] loss:0.3092 perplex:1.35\n",
      "----------\n",
      "[128/1000] loss:0.3148 perplex:1.34\n",
      "----------\n",
      "[129/1000] loss:0.2985 perplex:1.33\n",
      "----------\n",
      "[130/1000] loss:0.3014 perplex:1.32\n",
      "----------\n",
      "[131/1000] loss:0.2934 perplex:1.31\n",
      "----------\n",
      "[132/1000] loss:0.2951 perplex:1.30\n",
      "----------\n",
      "[133/1000] loss:0.2836 perplex:1.30\n",
      "----------\n",
      "[134/1000] loss:0.2747 perplex:1.30\n",
      "----------\n",
      "[135/1000] loss:0.2762 perplex:1.29\n",
      "----------\n",
      "[136/1000] loss:0.2555 perplex:1.28\n",
      "----------\n",
      "[137/1000] loss:0.2483 perplex:1.28\n",
      "----------\n",
      "[138/1000] loss:0.2509 perplex:1.27\n",
      "----------\n",
      "[139/1000] loss:0.2473 perplex:1.26\n",
      "----------\n",
      "[140/1000] loss:0.2504 perplex:1.25\n",
      "----------\n",
      "[141/1000] loss:0.2377 perplex:1.25\n",
      "----------\n",
      "[142/1000] loss:0.2321 perplex:1.25\n",
      "----------\n",
      "[143/1000] loss:0.2327 perplex:1.24\n",
      "----------\n",
      "[144/1000] loss:0.2275 perplex:1.23\n",
      "----------\n",
      "[145/1000] loss:0.2230 perplex:1.23\n",
      "----------\n",
      "[146/1000] loss:0.2184 perplex:1.22\n",
      "----------\n",
      "[147/1000] loss:0.2108 perplex:1.22\n",
      "----------\n",
      "[148/1000] loss:0.2118 perplex:1.22\n",
      "----------\n",
      "[149/1000] loss:0.2085 perplex:1.21\n",
      "----------\n",
      "[150/1000] loss:0.2074 perplex:1.21\n",
      "----------\n",
      "[151/1000] loss:0.2061 perplex:1.21\n",
      "----------\n",
      "[152/1000] loss:0.2106 perplex:1.21\n",
      "----------\n",
      "[153/1000] loss:0.2088 perplex:1.21\n",
      "----------\n",
      "[154/1000] loss:0.2087 perplex:1.21\n",
      "----------\n",
      "[155/1000] loss:0.2013 perplex:1.21\n",
      "----------\n",
      "[156/1000] loss:0.2009 perplex:1.20\n",
      "----------\n",
      "[157/1000] loss:0.1984 perplex:1.21\n",
      "----------\n",
      "[158/1000] loss:0.2155 perplex:1.22\n",
      "----------\n",
      "[159/1000] loss:0.2309 perplex:1.25\n",
      "----------\n",
      "[160/1000] loss:0.2632 perplex:1.27\n",
      "----------\n",
      "[161/1000] loss:0.2832 perplex:1.32\n",
      "----------\n",
      "[162/1000] loss:0.3420 perplex:1.37\n",
      "----------\n",
      "[163/1000] loss:0.4132 perplex:1.47\n",
      "----------\n",
      "[164/1000] loss:0.4696 perplex:1.56\n",
      "----------\n",
      "[165/1000] loss:0.5366 perplex:1.60\n",
      "----------\n",
      "[166/1000] loss:0.5438 perplex:1.67\n",
      "----------\n",
      "[167/1000] loss:0.5153 perplex:1.65\n",
      "----------\n",
      "[168/1000] loss:0.4418 perplex:1.53\n",
      "----------\n",
      "[169/1000] loss:0.3851 perplex:1.44\n",
      "----------\n",
      "[170/1000] loss:0.3470 perplex:1.37\n",
      "----------\n",
      "[171/1000] loss:0.2937 perplex:1.32\n",
      "----------\n",
      "[172/1000] loss:0.2619 perplex:1.27\n",
      "----------\n",
      "[173/1000] loss:0.2160 perplex:1.23\n",
      "----------\n",
      "[174/1000] loss:0.1938 perplex:1.20\n",
      "----------\n",
      "[175/1000] loss:0.1769 perplex:1.19\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[176/1000] loss:0.1699 perplex:1.18\n",
      "----------\n",
      "[177/1000] loss:0.1645 perplex:1.17\n",
      "----------\n",
      "[178/1000] loss:0.1575 perplex:1.16\n",
      "----------\n",
      "[179/1000] loss:0.1547 perplex:1.16\n",
      "----------\n",
      "[180/1000] loss:0.1551 perplex:1.16\n",
      "----------\n",
      "[181/1000] loss:0.1538 perplex:1.15\n",
      "----------\n",
      "[182/1000] loss:0.1481 perplex:1.15\n",
      "----------\n",
      "[183/1000] loss:0.1512 perplex:1.15\n",
      "----------\n",
      "[184/1000] loss:0.1435 perplex:1.15\n",
      "----------\n",
      "[185/1000] loss:0.1476 perplex:1.14\n",
      "----------\n",
      "[186/1000] loss:0.1444 perplex:1.14\n",
      "----------\n",
      "[187/1000] loss:0.1410 perplex:1.14\n",
      "----------\n",
      "[188/1000] loss:0.1391 perplex:1.14\n",
      "----------\n",
      "[189/1000] loss:0.1383 perplex:1.14\n",
      "----------\n",
      "[190/1000] loss:0.1370 perplex:1.14\n",
      "----------\n",
      "[191/1000] loss:0.1358 perplex:1.14\n",
      "----------\n",
      "[192/1000] loss:0.1355 perplex:1.14\n",
      "----------\n",
      "[193/1000] loss:0.1383 perplex:1.13\n",
      "----------\n",
      "[194/1000] loss:0.1365 perplex:1.13\n",
      "----------\n",
      "[195/1000] loss:0.1332 perplex:1.13\n",
      "----------\n",
      "[196/1000] loss:0.1342 perplex:1.13\n",
      "----------\n",
      "[197/1000] loss:0.1320 perplex:1.13\n",
      "----------\n",
      "[198/1000] loss:0.1332 perplex:1.13\n",
      "----------\n",
      "[199/1000] loss:0.1292 perplex:1.13\n",
      "----------\n",
      "[200/1000] loss:0.1344 perplex:1.13\n",
      "----------\n",
      "[201/1000] loss:0.1253 perplex:1.13\n",
      "----------\n",
      "[202/1000] loss:0.1328 perplex:1.13\n",
      "----------\n",
      "[203/1000] loss:0.1269 perplex:1.13\n",
      "----------\n",
      "[204/1000] loss:0.1306 perplex:1.13\n",
      "----------\n",
      "[205/1000] loss:0.1287 perplex:1.13\n",
      "----------\n",
      "[206/1000] loss:0.1276 perplex:1.13\n",
      "----------\n",
      "[207/1000] loss:0.1266 perplex:1.13\n",
      "----------\n",
      "[208/1000] loss:0.1230 perplex:1.12\n",
      "----------\n",
      "[209/1000] loss:0.1243 perplex:1.12\n",
      "----------\n",
      "[210/1000] loss:0.1267 perplex:1.12\n",
      "----------\n",
      "[211/1000] loss:0.1289 perplex:1.12\n",
      "----------\n",
      "[212/1000] loss:0.1267 perplex:1.12\n",
      "----------\n",
      "[213/1000] loss:0.1232 perplex:1.12\n",
      "----------\n",
      "[214/1000] loss:0.1269 perplex:1.12\n",
      "----------\n",
      "[215/1000] loss:0.1184 perplex:1.12\n",
      "----------\n",
      "[216/1000] loss:0.1205 perplex:1.12\n",
      "----------\n",
      "[217/1000] loss:0.1177 perplex:1.12\n",
      "----------\n",
      "[218/1000] loss:0.1163 perplex:1.12\n",
      "----------\n",
      "[219/1000] loss:0.1194 perplex:1.12\n",
      "----------\n",
      "[220/1000] loss:0.1207 perplex:1.12\n",
      "----------\n",
      "[221/1000] loss:0.1209 perplex:1.12\n",
      "----------\n",
      "[222/1000] loss:0.1223 perplex:1.12\n",
      "----------\n",
      "[223/1000] loss:0.1197 perplex:1.12\n",
      "----------\n",
      "[224/1000] loss:0.1198 perplex:1.12\n",
      "----------\n",
      "[225/1000] loss:0.1223 perplex:1.12\n",
      "----------\n",
      "[226/1000] loss:0.1211 perplex:1.12\n",
      "----------\n",
      "[227/1000] loss:0.1173 perplex:1.12\n",
      "----------\n",
      "[228/1000] loss:0.1196 perplex:1.12\n",
      "----------\n",
      "[229/1000] loss:0.1198 perplex:1.12\n",
      "----------\n",
      "[230/1000] loss:0.1214 perplex:1.12\n",
      "----------\n",
      "[231/1000] loss:0.1176 perplex:1.12\n",
      "----------\n",
      "[232/1000] loss:0.1181 perplex:1.12\n",
      "----------\n",
      "[233/1000] loss:0.1158 perplex:1.12\n",
      "----------\n",
      "[234/1000] loss:0.1179 perplex:1.12\n",
      "----------\n",
      "[235/1000] loss:0.1201 perplex:1.11\n",
      "----------\n",
      "[236/1000] loss:0.1171 perplex:1.11\n",
      "----------\n",
      "[237/1000] loss:0.1152 perplex:1.11\n",
      "----------\n",
      "[238/1000] loss:0.1153 perplex:1.11\n",
      "----------\n",
      "[239/1000] loss:0.1158 perplex:1.11\n",
      "----------\n",
      "[240/1000] loss:0.1183 perplex:1.11\n",
      "----------\n",
      "[241/1000] loss:0.1136 perplex:1.11\n",
      "----------\n",
      "[242/1000] loss:0.1154 perplex:1.11\n",
      "----------\n",
      "[243/1000] loss:0.1144 perplex:1.11\n",
      "----------\n",
      "[244/1000] loss:0.1133 perplex:1.11\n",
      "----------\n",
      "[245/1000] loss:0.1158 perplex:1.11\n",
      "----------\n",
      "[246/1000] loss:0.1107 perplex:1.11\n",
      "----------\n",
      "[247/1000] loss:0.1137 perplex:1.11\n",
      "----------\n",
      "[248/1000] loss:0.1152 perplex:1.11\n",
      "----------\n",
      "[249/1000] loss:0.1171 perplex:1.11\n",
      "----------\n",
      "[250/1000] loss:0.1079 perplex:1.11\n",
      "----------\n",
      "[251/1000] loss:0.1115 perplex:1.11\n",
      "----------\n",
      "[252/1000] loss:0.1130 perplex:1.11\n",
      "----------\n",
      "[253/1000] loss:0.1106 perplex:1.11\n",
      "----------\n",
      "[254/1000] loss:0.1099 perplex:1.11\n",
      "----------\n",
      "[255/1000] loss:0.1114 perplex:1.11\n",
      "----------\n",
      "[256/1000] loss:0.1074 perplex:1.11\n",
      "----------\n",
      "[257/1000] loss:0.1085 perplex:1.11\n",
      "----------\n",
      "[258/1000] loss:0.1072 perplex:1.11\n",
      "----------\n",
      "[259/1000] loss:0.1137 perplex:1.11\n",
      "----------\n",
      "[260/1000] loss:0.1089 perplex:1.11\n",
      "----------\n",
      "[261/1000] loss:0.1094 perplex:1.11\n",
      "----------\n",
      "[262/1000] loss:0.1084 perplex:1.11\n",
      "----------\n",
      "[263/1000] loss:0.1129 perplex:1.11\n",
      "----------\n",
      "[264/1000] loss:0.1120 perplex:1.11\n",
      "----------\n",
      "[265/1000] loss:0.1107 perplex:1.11\n",
      "----------\n",
      "[266/1000] loss:0.1114 perplex:1.11\n",
      "----------\n",
      "[267/1000] loss:0.1101 perplex:1.11\n",
      "----------\n",
      "[268/1000] loss:0.1110 perplex:1.11\n",
      "----------\n",
      "[269/1000] loss:0.1121 perplex:1.11\n",
      "----------\n",
      "[270/1000] loss:0.1074 perplex:1.11\n",
      "----------\n",
      "[271/1000] loss:0.1160 perplex:1.11\n",
      "----------\n",
      "[272/1000] loss:0.1080 perplex:1.11\n",
      "----------\n",
      "[273/1000] loss:0.1054 perplex:1.11\n",
      "----------\n",
      "[274/1000] loss:0.1075 perplex:1.11\n",
      "----------\n",
      "[275/1000] loss:0.1084 perplex:1.11\n",
      "----------\n",
      "[276/1000] loss:0.1092 perplex:1.11\n",
      "----------\n",
      "[277/1000] loss:0.1108 perplex:1.11\n",
      "----------\n",
      "[278/1000] loss:0.1096 perplex:1.11\n",
      "----------\n",
      "[279/1000] loss:0.1080 perplex:1.11\n",
      "----------\n",
      "[280/1000] loss:0.1060 perplex:1.11\n",
      "----------\n",
      "[281/1000] loss:0.1074 perplex:1.11\n",
      "----------\n",
      "[282/1000] loss:0.1057 perplex:1.11\n",
      "----------\n",
      "[283/1000] loss:0.1081 perplex:1.11\n",
      "----------\n",
      "[284/1000] loss:0.1079 perplex:1.11\n",
      "----------\n",
      "[285/1000] loss:0.1054 perplex:1.11\n",
      "----------\n",
      "[286/1000] loss:0.1066 perplex:1.11\n",
      "----------\n",
      "[287/1000] loss:0.1091 perplex:1.11\n",
      "----------\n",
      "[288/1000] loss:0.1074 perplex:1.11\n",
      "----------\n",
      "[289/1000] loss:0.1065 perplex:1.10\n",
      "----------\n",
      "[290/1000] loss:0.1083 perplex:1.10\n",
      "----------\n",
      "[291/1000] loss:0.1083 perplex:1.10\n",
      "----------\n",
      "[292/1000] loss:0.1051 perplex:1.10\n",
      "----------\n",
      "[293/1000] loss:0.1050 perplex:1.10\n",
      "----------\n",
      "[294/1000] loss:0.1096 perplex:1.10\n",
      "----------\n",
      "[295/1000] loss:0.1103 perplex:1.10\n",
      "----------\n",
      "[296/1000] loss:0.1016 perplex:1.10\n",
      "----------\n",
      "[297/1000] loss:0.1018 perplex:1.10\n",
      "----------\n",
      "[298/1000] loss:0.1038 perplex:1.10\n",
      "----------\n",
      "[299/1000] loss:0.1058 perplex:1.10\n",
      "----------\n",
      "[300/1000] loss:0.1070 perplex:1.10\n",
      "----------\n",
      "[301/1000] loss:0.1059 perplex:1.10\n",
      "----------\n",
      "[302/1000] loss:0.1079 perplex:1.10\n",
      "----------\n",
      "[303/1000] loss:0.1043 perplex:1.10\n",
      "----------\n",
      "[304/1000] loss:0.1039 perplex:1.10\n",
      "----------\n",
      "[305/1000] loss:0.1084 perplex:1.10\n",
      "----------\n",
      "[306/1000] loss:0.1091 perplex:1.10\n",
      "----------\n",
      "[307/1000] loss:0.1043 perplex:1.10\n",
      "----------\n",
      "[308/1000] loss:0.1096 perplex:1.10\n",
      "----------\n",
      "[309/1000] loss:0.1040 perplex:1.10\n",
      "----------\n",
      "[310/1000] loss:0.1011 perplex:1.10\n",
      "----------\n",
      "[311/1000] loss:0.1058 perplex:1.10\n",
      "----------\n",
      "[312/1000] loss:0.1073 perplex:1.10\n",
      "----------\n",
      "[313/1000] loss:0.1079 perplex:1.10\n",
      "----------\n",
      "[314/1000] loss:0.1032 perplex:1.10\n",
      "----------\n",
      "[315/1000] loss:0.1063 perplex:1.10\n",
      "----------\n",
      "[316/1000] loss:0.1084 perplex:1.10\n",
      "----------\n",
      "[317/1000] loss:0.0996 perplex:1.10\n",
      "----------\n",
      "[318/1000] loss:0.1069 perplex:1.10\n",
      "----------\n",
      "[319/1000] loss:0.1030 perplex:1.10\n",
      "----------\n",
      "[320/1000] loss:0.1109 perplex:1.10\n",
      "----------\n",
      "[321/1000] loss:0.1091 perplex:1.10\n",
      "----------\n",
      "[322/1000] loss:0.1037 perplex:1.10\n",
      "----------\n",
      "[323/1000] loss:0.1061 perplex:1.10\n",
      "----------\n",
      "[324/1000] loss:0.1051 perplex:1.10\n",
      "----------\n",
      "[325/1000] loss:0.1043 perplex:1.10\n",
      "----------\n",
      "[326/1000] loss:0.1047 perplex:1.10\n",
      "----------\n",
      "[327/1000] loss:0.1065 perplex:1.10\n",
      "----------\n",
      "[328/1000] loss:0.1038 perplex:1.10\n",
      "----------\n",
      "[329/1000] loss:0.1012 perplex:1.10\n",
      "----------\n",
      "[330/1000] loss:0.0996 perplex:1.10\n",
      "----------\n",
      "[331/1000] loss:0.1025 perplex:1.10\n",
      "----------\n",
      "[332/1000] loss:0.1008 perplex:1.10\n",
      "----------\n",
      "[333/1000] loss:0.1009 perplex:1.10\n",
      "----------\n",
      "[334/1000] loss:0.1033 perplex:1.10\n",
      "----------\n",
      "[335/1000] loss:0.1031 perplex:1.10\n",
      "----------\n",
      "[336/1000] loss:0.1054 perplex:1.10\n",
      "----------\n",
      "[337/1000] loss:0.1047 perplex:1.10\n",
      "----------\n",
      "[338/1000] loss:0.0993 perplex:1.10\n",
      "----------\n",
      "[339/1000] loss:0.1038 perplex:1.10\n",
      "----------\n",
      "[340/1000] loss:0.1015 perplex:1.10\n",
      "----------\n",
      "[341/1000] loss:0.1018 perplex:1.10\n",
      "----------\n",
      "[342/1000] loss:0.1083 perplex:1.10\n",
      "----------\n",
      "[343/1000] loss:0.1021 perplex:1.10\n",
      "----------\n",
      "[344/1000] loss:0.0991 perplex:1.10\n",
      "----------\n",
      "[345/1000] loss:0.1038 perplex:1.10\n",
      "----------\n",
      "[346/1000] loss:0.1037 perplex:1.10\n",
      "----------\n",
      "[347/1000] loss:0.0998 perplex:1.10\n",
      "----------\n",
      "[348/1000] loss:0.1073 perplex:1.10\n",
      "----------\n",
      "[349/1000] loss:0.1016 perplex:1.10\n",
      "----------\n",
      "[350/1000] loss:0.1032 perplex:1.10\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[351/1000] loss:0.0997 perplex:1.10\n",
      "----------\n",
      "[352/1000] loss:0.1033 perplex:1.10\n",
      "----------\n",
      "[353/1000] loss:0.1031 perplex:1.10\n",
      "----------\n",
      "[354/1000] loss:0.0995 perplex:1.10\n",
      "----------\n",
      "[355/1000] loss:0.1019 perplex:1.10\n",
      "----------\n",
      "[356/1000] loss:0.1020 perplex:1.10\n",
      "----------\n",
      "[357/1000] loss:0.1017 perplex:1.10\n",
      "----------\n",
      "[358/1000] loss:0.0988 perplex:1.10\n",
      "----------\n",
      "[359/1000] loss:0.1039 perplex:1.10\n",
      "----------\n",
      "[360/1000] loss:0.1037 perplex:1.10\n",
      "----------\n",
      "[361/1000] loss:0.1031 perplex:1.10\n",
      "----------\n",
      "[362/1000] loss:0.1018 perplex:1.10\n",
      "----------\n",
      "[363/1000] loss:0.0966 perplex:1.10\n",
      "----------\n",
      "[364/1000] loss:0.0984 perplex:1.10\n",
      "----------\n",
      "[365/1000] loss:0.1019 perplex:1.10\n",
      "----------\n",
      "[366/1000] loss:0.1070 perplex:1.10\n",
      "----------\n",
      "[367/1000] loss:0.1056 perplex:1.10\n",
      "----------\n",
      "[368/1000] loss:0.1025 perplex:1.10\n",
      "----------\n",
      "[369/1000] loss:0.1010 perplex:1.10\n",
      "----------\n",
      "[370/1000] loss:0.1015 perplex:1.10\n",
      "----------\n",
      "[371/1000] loss:0.1028 perplex:1.10\n",
      "----------\n",
      "[372/1000] loss:0.1035 perplex:1.10\n",
      "----------\n",
      "[373/1000] loss:0.0987 perplex:1.10\n",
      "----------\n",
      "[374/1000] loss:0.0988 perplex:1.10\n",
      "----------\n",
      "[375/1000] loss:0.1048 perplex:1.10\n",
      "----------\n",
      "[376/1000] loss:0.1006 perplex:1.10\n",
      "----------\n",
      "[377/1000] loss:0.1017 perplex:1.10\n",
      "----------\n",
      "[378/1000] loss:0.0991 perplex:1.10\n",
      "----------\n",
      "[379/1000] loss:0.1020 perplex:1.10\n",
      "----------\n",
      "[380/1000] loss:0.1004 perplex:1.10\n",
      "----------\n",
      "[381/1000] loss:0.0986 perplex:1.10\n",
      "----------\n",
      "[382/1000] loss:0.0995 perplex:1.10\n",
      "----------\n",
      "[383/1000] loss:0.0962 perplex:1.10\n",
      "----------\n",
      "[384/1000] loss:0.0996 perplex:1.10\n",
      "----------\n",
      "[385/1000] loss:0.1008 perplex:1.10\n",
      "----------\n",
      "[386/1000] loss:0.0963 perplex:1.10\n",
      "----------\n",
      "[387/1000] loss:0.1029 perplex:1.10\n",
      "----------\n",
      "[388/1000] loss:0.1010 perplex:1.10\n",
      "----------\n",
      "[389/1000] loss:0.1018 perplex:1.10\n",
      "----------\n",
      "[390/1000] loss:0.0987 perplex:1.10\n",
      "----------\n",
      "[391/1000] loss:0.0962 perplex:1.10\n",
      "----------\n",
      "[392/1000] loss:0.1005 perplex:1.10\n",
      "----------\n",
      "[393/1000] loss:0.1044 perplex:1.10\n",
      "----------\n",
      "[394/1000] loss:0.0985 perplex:1.10\n",
      "----------\n",
      "[395/1000] loss:0.0981 perplex:1.10\n",
      "----------\n",
      "[396/1000] loss:0.0972 perplex:1.10\n",
      "----------\n",
      "[397/1000] loss:0.1006 perplex:1.10\n",
      "----------\n",
      "[398/1000] loss:0.0985 perplex:1.10\n",
      "----------\n",
      "[399/1000] loss:0.1006 perplex:1.10\n",
      "----------\n",
      "[400/1000] loss:0.1018 perplex:1.10\n",
      "----------\n",
      "[401/1000] loss:0.1066 perplex:1.10\n",
      "----------\n",
      "[402/1000] loss:0.1643 perplex:1.16\n",
      "----------\n",
      "[403/1000] loss:0.7890 perplex:2.02\n",
      "----------\n",
      "[404/1000] loss:1.5920 perplex:4.28\n",
      "----------\n",
      "[405/1000] loss:1.5867 perplex:4.51\n",
      "----------\n",
      "[406/1000] loss:1.4055 perplex:3.60\n",
      "----------\n",
      "[407/1000] loss:1.0779 perplex:2.76\n",
      "----------\n",
      "[408/1000] loss:0.8638 perplex:2.18\n",
      "----------\n",
      "[409/1000] loss:0.6347 perplex:1.80\n",
      "----------\n",
      "[410/1000] loss:0.4767 perplex:1.55\n",
      "----------\n",
      "[411/1000] loss:0.3702 perplex:1.39\n",
      "----------\n",
      "[412/1000] loss:0.2657 perplex:1.29\n",
      "----------\n",
      "[413/1000] loss:0.2181 perplex:1.22\n",
      "----------\n",
      "[414/1000] loss:0.1812 perplex:1.18\n",
      "----------\n",
      "[415/1000] loss:0.1532 perplex:1.16\n",
      "----------\n",
      "[416/1000] loss:0.1436 perplex:1.14\n",
      "----------\n",
      "[417/1000] loss:0.1286 perplex:1.13\n",
      "----------\n",
      "[418/1000] loss:0.1241 perplex:1.13\n",
      "----------\n",
      "[419/1000] loss:0.1299 perplex:1.12\n",
      "----------\n",
      "[420/1000] loss:0.1191 perplex:1.12\n",
      "----------\n",
      "[421/1000] loss:0.1203 perplex:1.12\n",
      "----------\n",
      "[422/1000] loss:0.1165 perplex:1.12\n",
      "----------\n",
      "[423/1000] loss:0.1189 perplex:1.12\n",
      "----------\n",
      "[424/1000] loss:0.1132 perplex:1.11\n",
      "----------\n",
      "[425/1000] loss:0.1204 perplex:1.11\n",
      "----------\n",
      "[426/1000] loss:0.1166 perplex:1.11\n",
      "----------\n",
      "[427/1000] loss:0.1144 perplex:1.11\n",
      "----------\n",
      "[428/1000] loss:0.1151 perplex:1.11\n",
      "----------\n",
      "[429/1000] loss:0.1125 perplex:1.11\n",
      "----------\n",
      "[430/1000] loss:0.1132 perplex:1.11\n",
      "----------\n",
      "[431/1000] loss:0.1149 perplex:1.11\n",
      "----------\n",
      "[432/1000] loss:0.1099 perplex:1.11\n",
      "----------\n",
      "[433/1000] loss:0.1114 perplex:1.11\n",
      "----------\n",
      "[434/1000] loss:0.1072 perplex:1.11\n",
      "----------\n",
      "[435/1000] loss:0.1056 perplex:1.11\n",
      "----------\n",
      "[436/1000] loss:0.1043 perplex:1.11\n",
      "----------\n",
      "[437/1000] loss:0.1040 perplex:1.11\n",
      "----------\n",
      "[438/1000] loss:0.1037 perplex:1.11\n",
      "----------\n",
      "[439/1000] loss:0.1079 perplex:1.11\n",
      "----------\n",
      "[440/1000] loss:0.1091 perplex:1.11\n",
      "----------\n",
      "[441/1000] loss:0.1050 perplex:1.11\n",
      "----------\n",
      "[442/1000] loss:0.1045 perplex:1.11\n",
      "----------\n",
      "[443/1000] loss:0.1065 perplex:1.11\n",
      "----------\n",
      "[444/1000] loss:0.1037 perplex:1.10\n",
      "----------\n",
      "[445/1000] loss:0.1097 perplex:1.10\n",
      "----------\n",
      "[446/1000] loss:0.1069 perplex:1.10\n",
      "----------\n",
      "[447/1000] loss:0.1039 perplex:1.10\n",
      "----------\n",
      "[448/1000] loss:0.1068 perplex:1.10\n",
      "----------\n",
      "[449/1000] loss:0.1030 perplex:1.10\n",
      "----------\n",
      "[450/1000] loss:0.1089 perplex:1.10\n",
      "----------\n",
      "[451/1000] loss:0.1079 perplex:1.10\n",
      "----------\n",
      "[452/1000] loss:0.1090 perplex:1.10\n",
      "----------\n",
      "[453/1000] loss:0.1067 perplex:1.10\n",
      "----------\n",
      "[454/1000] loss:0.1051 perplex:1.10\n",
      "----------\n",
      "[455/1000] loss:0.1092 perplex:1.10\n",
      "----------\n",
      "[456/1000] loss:0.1021 perplex:1.10\n",
      "----------\n",
      "[457/1000] loss:0.1050 perplex:1.10\n",
      "----------\n",
      "[458/1000] loss:0.1034 perplex:1.10\n",
      "----------\n",
      "[459/1000] loss:0.1017 perplex:1.10\n",
      "----------\n",
      "[460/1000] loss:0.1020 perplex:1.10\n",
      "----------\n",
      "[461/1000] loss:0.1055 perplex:1.10\n",
      "----------\n",
      "[462/1000] loss:0.1067 perplex:1.10\n",
      "----------\n",
      "[463/1000] loss:0.1014 perplex:1.10\n",
      "----------\n",
      "[464/1000] loss:0.1038 perplex:1.10\n",
      "----------\n",
      "[465/1000] loss:0.1049 perplex:1.10\n",
      "----------\n",
      "[466/1000] loss:0.1023 perplex:1.10\n",
      "----------\n",
      "[467/1000] loss:0.1021 perplex:1.10\n",
      "----------\n",
      "[468/1000] loss:0.1017 perplex:1.10\n",
      "----------\n",
      "[469/1000] loss:0.1025 perplex:1.10\n",
      "----------\n",
      "[470/1000] loss:0.1054 perplex:1.10\n",
      "----------\n",
      "[471/1000] loss:0.1033 perplex:1.10\n",
      "----------\n",
      "[472/1000] loss:0.1072 perplex:1.10\n",
      "----------\n",
      "[473/1000] loss:0.1035 perplex:1.10\n",
      "----------\n",
      "[474/1000] loss:0.1034 perplex:1.10\n",
      "----------\n",
      "[475/1000] loss:0.1032 perplex:1.10\n",
      "----------\n",
      "[476/1000] loss:0.1052 perplex:1.10\n",
      "----------\n",
      "[477/1000] loss:0.0995 perplex:1.10\n",
      "----------\n",
      "[478/1000] loss:0.1058 perplex:1.10\n",
      "----------\n",
      "[479/1000] loss:0.1092 perplex:1.10\n",
      "----------\n",
      "[480/1000] loss:0.1010 perplex:1.10\n",
      "----------\n",
      "[481/1000] loss:0.1033 perplex:1.10\n",
      "----------\n",
      "[482/1000] loss:0.1005 perplex:1.10\n",
      "----------\n",
      "[483/1000] loss:0.1006 perplex:1.10\n",
      "----------\n",
      "[484/1000] loss:0.0993 perplex:1.10\n",
      "----------\n",
      "[485/1000] loss:0.1004 perplex:1.10\n",
      "----------\n",
      "[486/1000] loss:0.1035 perplex:1.10\n",
      "----------\n",
      "[487/1000] loss:0.1027 perplex:1.10\n",
      "----------\n",
      "[488/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[489/1000] loss:0.0972 perplex:1.10\n",
      "----------\n",
      "[490/1000] loss:0.0996 perplex:1.10\n",
      "----------\n",
      "[491/1000] loss:0.1014 perplex:1.10\n",
      "----------\n",
      "[492/1000] loss:0.0990 perplex:1.10\n",
      "----------\n",
      "[493/1000] loss:0.1022 perplex:1.10\n",
      "----------\n",
      "[494/1000] loss:0.1027 perplex:1.10\n",
      "----------\n",
      "[495/1000] loss:0.0998 perplex:1.10\n",
      "----------\n",
      "[496/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[497/1000] loss:0.0996 perplex:1.10\n",
      "----------\n",
      "[498/1000] loss:0.1034 perplex:1.10\n",
      "----------\n",
      "[499/1000] loss:0.1001 perplex:1.10\n",
      "----------\n",
      "[500/1000] loss:0.1008 perplex:1.10\n",
      "----------\n",
      "[501/1000] loss:0.0991 perplex:1.10\n",
      "----------\n",
      "[502/1000] loss:0.0991 perplex:1.10\n",
      "----------\n",
      "[503/1000] loss:0.0968 perplex:1.10\n",
      "----------\n",
      "[504/1000] loss:0.1088 perplex:1.10\n",
      "----------\n",
      "[505/1000] loss:0.0967 perplex:1.10\n",
      "----------\n",
      "[506/1000] loss:0.0970 perplex:1.10\n",
      "----------\n",
      "[507/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[508/1000] loss:0.1014 perplex:1.10\n",
      "----------\n",
      "[509/1000] loss:0.0959 perplex:1.10\n",
      "----------\n",
      "[510/1000] loss:0.1032 perplex:1.10\n",
      "----------\n",
      "[511/1000] loss:0.0998 perplex:1.10\n",
      "----------\n",
      "[512/1000] loss:0.0985 perplex:1.10\n",
      "----------\n",
      "[513/1000] loss:0.1022 perplex:1.10\n",
      "----------\n",
      "[514/1000] loss:0.0985 perplex:1.10\n",
      "----------\n",
      "[515/1000] loss:0.0987 perplex:1.10\n",
      "----------\n",
      "[516/1000] loss:0.1013 perplex:1.10\n",
      "----------\n",
      "[517/1000] loss:0.1022 perplex:1.10\n",
      "----------\n",
      "[518/1000] loss:0.0986 perplex:1.10\n",
      "----------\n",
      "[519/1000] loss:0.0998 perplex:1.10\n",
      "----------\n",
      "[520/1000] loss:0.1040 perplex:1.10\n",
      "----------\n",
      "[521/1000] loss:0.1042 perplex:1.10\n",
      "----------\n",
      "[522/1000] loss:0.0994 perplex:1.10\n",
      "----------\n",
      "[523/1000] loss:0.1006 perplex:1.10\n",
      "----------\n",
      "[524/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[525/1000] loss:0.0973 perplex:1.10\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[526/1000] loss:0.0979 perplex:1.10\n",
      "----------\n",
      "[527/1000] loss:0.0998 perplex:1.10\n",
      "----------\n",
      "[528/1000] loss:0.1013 perplex:1.10\n",
      "----------\n",
      "[529/1000] loss:0.0951 perplex:1.10\n",
      "----------\n",
      "[530/1000] loss:0.0971 perplex:1.10\n",
      "----------\n",
      "[531/1000] loss:0.1002 perplex:1.10\n",
      "----------\n",
      "[532/1000] loss:0.1061 perplex:1.10\n",
      "----------\n",
      "[533/1000] loss:0.0988 perplex:1.10\n",
      "----------\n",
      "[534/1000] loss:0.0979 perplex:1.10\n",
      "----------\n",
      "[535/1000] loss:0.0977 perplex:1.10\n",
      "----------\n",
      "[536/1000] loss:0.1008 perplex:1.10\n",
      "----------\n",
      "[537/1000] loss:0.0983 perplex:1.10\n",
      "----------\n",
      "[538/1000] loss:0.1018 perplex:1.10\n",
      "----------\n",
      "[539/1000] loss:0.1026 perplex:1.10\n",
      "----------\n",
      "[540/1000] loss:0.0969 perplex:1.10\n",
      "----------\n",
      "[541/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[542/1000] loss:0.1003 perplex:1.10\n",
      "----------\n",
      "[543/1000] loss:0.0949 perplex:1.10\n",
      "----------\n",
      "[544/1000] loss:0.0950 perplex:1.10\n",
      "----------\n",
      "[545/1000] loss:0.1013 perplex:1.10\n",
      "----------\n",
      "[546/1000] loss:0.1001 perplex:1.10\n",
      "----------\n",
      "[547/1000] loss:0.1017 perplex:1.10\n",
      "----------\n",
      "[548/1000] loss:0.1000 perplex:1.10\n",
      "----------\n",
      "[549/1000] loss:0.0975 perplex:1.10\n",
      "----------\n",
      "[550/1000] loss:0.1048 perplex:1.10\n",
      "----------\n",
      "[551/1000] loss:0.0970 perplex:1.10\n",
      "----------\n",
      "[552/1000] loss:0.0971 perplex:1.10\n",
      "----------\n",
      "[553/1000] loss:0.0994 perplex:1.10\n",
      "----------\n",
      "[554/1000] loss:0.0948 perplex:1.10\n",
      "----------\n",
      "[555/1000] loss:0.0979 perplex:1.10\n",
      "----------\n",
      "[556/1000] loss:0.0997 perplex:1.10\n",
      "----------\n",
      "[557/1000] loss:0.0955 perplex:1.10\n",
      "----------\n",
      "[558/1000] loss:0.0997 perplex:1.10\n",
      "----------\n",
      "[559/1000] loss:0.0962 perplex:1.10\n",
      "----------\n",
      "[560/1000] loss:0.0998 perplex:1.10\n",
      "----------\n",
      "[561/1000] loss:0.1013 perplex:1.10\n",
      "----------\n",
      "[562/1000] loss:0.1002 perplex:1.10\n",
      "----------\n",
      "[563/1000] loss:0.0984 perplex:1.10\n",
      "----------\n",
      "[564/1000] loss:0.1014 perplex:1.10\n",
      "----------\n",
      "[565/1000] loss:0.0952 perplex:1.10\n",
      "----------\n",
      "[566/1000] loss:0.0960 perplex:1.10\n",
      "----------\n",
      "[567/1000] loss:0.0957 perplex:1.10\n",
      "----------\n",
      "[568/1000] loss:0.0933 perplex:1.10\n",
      "----------\n",
      "[569/1000] loss:0.1000 perplex:1.10\n",
      "----------\n",
      "[570/1000] loss:0.0968 perplex:1.10\n",
      "----------\n",
      "[571/1000] loss:0.0985 perplex:1.10\n",
      "----------\n",
      "[572/1000] loss:0.0984 perplex:1.10\n",
      "----------\n",
      "[573/1000] loss:0.1007 perplex:1.10\n",
      "----------\n",
      "[574/1000] loss:0.0965 perplex:1.10\n",
      "----------\n",
      "[575/1000] loss:0.0997 perplex:1.10\n",
      "----------\n",
      "[576/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[577/1000] loss:0.0982 perplex:1.10\n",
      "----------\n",
      "[578/1000] loss:0.0956 perplex:1.10\n",
      "----------\n",
      "[579/1000] loss:0.1056 perplex:1.10\n",
      "----------\n",
      "[580/1000] loss:0.0937 perplex:1.10\n",
      "----------\n",
      "[581/1000] loss:0.0963 perplex:1.10\n",
      "----------\n",
      "[582/1000] loss:0.1001 perplex:1.10\n",
      "----------\n",
      "[583/1000] loss:0.0982 perplex:1.10\n",
      "----------\n",
      "[584/1000] loss:0.0940 perplex:1.10\n",
      "----------\n",
      "[585/1000] loss:0.0950 perplex:1.10\n",
      "----------\n",
      "[586/1000] loss:0.1013 perplex:1.10\n",
      "----------\n",
      "[587/1000] loss:0.0956 perplex:1.10\n",
      "----------\n",
      "[588/1000] loss:0.0990 perplex:1.10\n",
      "----------\n",
      "[589/1000] loss:0.0980 perplex:1.10\n",
      "----------\n",
      "[590/1000] loss:0.0980 perplex:1.10\n",
      "----------\n",
      "[591/1000] loss:0.0984 perplex:1.10\n",
      "----------\n",
      "[592/1000] loss:0.0989 perplex:1.10\n",
      "----------\n",
      "[593/1000] loss:0.0943 perplex:1.10\n",
      "----------\n",
      "[594/1000] loss:0.0951 perplex:1.10\n",
      "----------\n",
      "[595/1000] loss:0.0965 perplex:1.10\n",
      "----------\n",
      "[596/1000] loss:0.0988 perplex:1.09\n",
      "----------\n",
      "[597/1000] loss:0.0981 perplex:1.10\n",
      "----------\n",
      "[598/1000] loss:0.0995 perplex:1.10\n",
      "----------\n",
      "[599/1000] loss:0.0992 perplex:1.10\n",
      "----------\n",
      "[600/1000] loss:0.0977 perplex:1.10\n",
      "----------\n",
      "[601/1000] loss:0.0971 perplex:1.10\n",
      "----------\n",
      "[602/1000] loss:0.0957 perplex:1.09\n",
      "----------\n",
      "[603/1000] loss:0.0952 perplex:1.10\n",
      "----------\n",
      "[604/1000] loss:0.0986 perplex:1.09\n",
      "----------\n",
      "[605/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[606/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[607/1000] loss:0.0969 perplex:1.09\n",
      "----------\n",
      "[608/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[609/1000] loss:0.1010 perplex:1.09\n",
      "----------\n",
      "[610/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[611/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[612/1000] loss:0.0954 perplex:1.09\n",
      "----------\n",
      "[613/1000] loss:0.0982 perplex:1.09\n",
      "----------\n",
      "[614/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[615/1000] loss:0.1008 perplex:1.09\n",
      "----------\n",
      "[616/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[617/1000] loss:0.0992 perplex:1.09\n",
      "----------\n",
      "[618/1000] loss:0.0954 perplex:1.09\n",
      "----------\n",
      "[619/1000] loss:0.0982 perplex:1.09\n",
      "----------\n",
      "[620/1000] loss:0.0983 perplex:1.09\n",
      "----------\n",
      "[621/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[622/1000] loss:0.0945 perplex:1.09\n",
      "----------\n",
      "[623/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[624/1000] loss:0.0925 perplex:1.09\n",
      "----------\n",
      "[625/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[626/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[627/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[628/1000] loss:0.0998 perplex:1.09\n",
      "----------\n",
      "[629/1000] loss:0.0972 perplex:1.09\n",
      "----------\n",
      "[630/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[631/1000] loss:0.0968 perplex:1.09\n",
      "----------\n",
      "[632/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[633/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[634/1000] loss:0.0951 perplex:1.09\n",
      "----------\n",
      "[635/1000] loss:0.0973 perplex:1.09\n",
      "----------\n",
      "[636/1000] loss:0.0962 perplex:1.09\n",
      "----------\n",
      "[637/1000] loss:0.0955 perplex:1.09\n",
      "----------\n",
      "[638/1000] loss:0.0969 perplex:1.09\n",
      "----------\n",
      "[639/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[640/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[641/1000] loss:0.0994 perplex:1.09\n",
      "----------\n",
      "[642/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[643/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[644/1000] loss:0.0942 perplex:1.09\n",
      "----------\n",
      "[645/1000] loss:0.1000 perplex:1.09\n",
      "----------\n",
      "[646/1000] loss:0.0996 perplex:1.09\n",
      "----------\n",
      "[647/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[648/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[649/1000] loss:0.0944 perplex:1.09\n",
      "----------\n",
      "[650/1000] loss:0.0967 perplex:1.09\n",
      "----------\n",
      "[651/1000] loss:0.0942 perplex:1.09\n",
      "----------\n",
      "[652/1000] loss:0.0953 perplex:1.09\n",
      "----------\n",
      "[653/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[654/1000] loss:0.0942 perplex:1.09\n",
      "----------\n",
      "[655/1000] loss:0.0967 perplex:1.09\n",
      "----------\n",
      "[656/1000] loss:0.1021 perplex:1.09\n",
      "----------\n",
      "[657/1000] loss:0.0967 perplex:1.09\n",
      "----------\n",
      "[658/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[659/1000] loss:0.0985 perplex:1.09\n",
      "----------\n",
      "[660/1000] loss:0.0978 perplex:1.09\n",
      "----------\n",
      "[661/1000] loss:0.0988 perplex:1.09\n",
      "----------\n",
      "[662/1000] loss:0.0978 perplex:1.09\n",
      "----------\n",
      "[663/1000] loss:0.0982 perplex:1.09\n",
      "----------\n",
      "[664/1000] loss:0.0983 perplex:1.09\n",
      "----------\n",
      "[665/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[666/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[667/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[668/1000] loss:0.1006 perplex:1.09\n",
      "----------\n",
      "[669/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[670/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[671/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[672/1000] loss:0.0920 perplex:1.09\n",
      "----------\n",
      "[673/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[674/1000] loss:0.0962 perplex:1.09\n",
      "----------\n",
      "[675/1000] loss:0.0966 perplex:1.09\n",
      "----------\n",
      "[676/1000] loss:0.0948 perplex:1.09\n",
      "----------\n",
      "[677/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[678/1000] loss:0.0932 perplex:1.09\n",
      "----------\n",
      "[679/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[680/1000] loss:0.0968 perplex:1.09\n",
      "----------\n",
      "[681/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[682/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[683/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[684/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[685/1000] loss:0.0987 perplex:1.09\n",
      "----------\n",
      "[686/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[687/1000] loss:0.0928 perplex:1.09\n",
      "----------\n",
      "[688/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[689/1000] loss:0.0976 perplex:1.09\n",
      "----------\n",
      "[690/1000] loss:0.0966 perplex:1.09\n",
      "----------\n",
      "[691/1000] loss:0.0996 perplex:1.09\n",
      "----------\n",
      "[692/1000] loss:0.0948 perplex:1.09\n",
      "----------\n",
      "[693/1000] loss:0.1016 perplex:1.09\n",
      "----------\n",
      "[694/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[695/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[696/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[697/1000] loss:0.1000 perplex:1.09\n",
      "----------\n",
      "[698/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[699/1000] loss:0.0939 perplex:1.09\n",
      "----------\n",
      "[700/1000] loss:0.0936 perplex:1.09\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[701/1000] loss:0.0997 perplex:1.09\n",
      "----------\n",
      "[702/1000] loss:0.0910 perplex:1.09\n",
      "----------\n",
      "[703/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[704/1000] loss:0.1002 perplex:1.09\n",
      "----------\n",
      "[705/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[706/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[707/1000] loss:0.0936 perplex:1.09\n",
      "----------\n",
      "[708/1000] loss:0.0935 perplex:1.09\n",
      "----------\n",
      "[709/1000] loss:0.0916 perplex:1.09\n",
      "----------\n",
      "[710/1000] loss:0.0986 perplex:1.09\n",
      "----------\n",
      "[711/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[712/1000] loss:0.0990 perplex:1.09\n",
      "----------\n",
      "[713/1000] loss:0.0944 perplex:1.09\n",
      "----------\n",
      "[714/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[715/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[716/1000] loss:0.0953 perplex:1.09\n",
      "----------\n",
      "[717/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[718/1000] loss:0.0992 perplex:1.09\n",
      "----------\n",
      "[719/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[720/1000] loss:0.0996 perplex:1.09\n",
      "----------\n",
      "[721/1000] loss:0.0957 perplex:1.09\n",
      "----------\n",
      "[722/1000] loss:0.0934 perplex:1.09\n",
      "----------\n",
      "[723/1000] loss:0.0993 perplex:1.09\n",
      "----------\n",
      "[724/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[725/1000] loss:0.1025 perplex:1.09\n",
      "----------\n",
      "[726/1000] loss:0.0998 perplex:1.09\n",
      "----------\n",
      "[727/1000] loss:0.0966 perplex:1.09\n",
      "----------\n",
      "[728/1000] loss:0.0943 perplex:1.09\n",
      "----------\n",
      "[729/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[730/1000] loss:0.0943 perplex:1.09\n",
      "----------\n",
      "[731/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[732/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[733/1000] loss:0.0978 perplex:1.09\n",
      "----------\n",
      "[734/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[735/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[736/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[737/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[738/1000] loss:0.0908 perplex:1.09\n",
      "----------\n",
      "[739/1000] loss:0.0984 perplex:1.09\n",
      "----------\n",
      "[740/1000] loss:0.0993 perplex:1.09\n",
      "----------\n",
      "[741/1000] loss:0.0944 perplex:1.09\n",
      "----------\n",
      "[742/1000] loss:0.0914 perplex:1.09\n",
      "----------\n",
      "[743/1000] loss:0.0992 perplex:1.09\n",
      "----------\n",
      "[744/1000] loss:0.0997 perplex:1.09\n",
      "----------\n",
      "[745/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[746/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[747/1000] loss:0.0939 perplex:1.09\n",
      "----------\n",
      "[748/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[749/1000] loss:0.0986 perplex:1.09\n",
      "----------\n",
      "[750/1000] loss:0.0986 perplex:1.09\n",
      "----------\n",
      "[751/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[752/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[753/1000] loss:0.0945 perplex:1.09\n",
      "----------\n",
      "[754/1000] loss:0.0985 perplex:1.09\n",
      "----------\n",
      "[755/1000] loss:0.0935 perplex:1.09\n",
      "----------\n",
      "[756/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[757/1000] loss:0.0982 perplex:1.09\n",
      "----------\n",
      "[758/1000] loss:0.0946 perplex:1.09\n",
      "----------\n",
      "[759/1000] loss:0.0973 perplex:1.09\n",
      "----------\n",
      "[760/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[761/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[762/1000] loss:0.0938 perplex:1.09\n",
      "----------\n",
      "[763/1000] loss:0.0932 perplex:1.09\n",
      "----------\n",
      "[764/1000] loss:0.0987 perplex:1.09\n",
      "----------\n",
      "[765/1000] loss:0.0993 perplex:1.09\n",
      "----------\n",
      "[766/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[767/1000] loss:0.0932 perplex:1.09\n",
      "----------\n",
      "[768/1000] loss:0.0973 perplex:1.09\n",
      "----------\n",
      "[769/1000] loss:0.0978 perplex:1.09\n",
      "----------\n",
      "[770/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[771/1000] loss:0.0934 perplex:1.09\n",
      "----------\n",
      "[772/1000] loss:0.0938 perplex:1.09\n",
      "----------\n",
      "[773/1000] loss:0.0942 perplex:1.09\n",
      "----------\n",
      "[774/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[775/1000] loss:0.0985 perplex:1.09\n",
      "----------\n",
      "[776/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[777/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[778/1000] loss:0.0931 perplex:1.09\n",
      "----------\n",
      "[779/1000] loss:0.0984 perplex:1.09\n",
      "----------\n",
      "[780/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[781/1000] loss:0.0969 perplex:1.09\n",
      "----------\n",
      "[782/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[783/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[784/1000] loss:0.0967 perplex:1.09\n",
      "----------\n",
      "[785/1000] loss:0.0912 perplex:1.09\n",
      "----------\n",
      "[786/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[787/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[788/1000] loss:0.0994 perplex:1.09\n",
      "----------\n",
      "[789/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[790/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[791/1000] loss:0.1017 perplex:1.09\n",
      "----------\n",
      "[792/1000] loss:0.0943 perplex:1.09\n",
      "----------\n",
      "[793/1000] loss:0.0913 perplex:1.09\n",
      "----------\n",
      "[794/1000] loss:0.1021 perplex:1.09\n",
      "----------\n",
      "[795/1000] loss:0.0966 perplex:1.09\n",
      "----------\n",
      "[796/1000] loss:0.0973 perplex:1.09\n",
      "----------\n",
      "[797/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[798/1000] loss:0.0942 perplex:1.09\n",
      "----------\n",
      "[799/1000] loss:0.0945 perplex:1.09\n",
      "----------\n",
      "[800/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[801/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[802/1000] loss:0.0918 perplex:1.09\n",
      "----------\n",
      "[803/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[804/1000] loss:0.0966 perplex:1.09\n",
      "----------\n",
      "[805/1000] loss:0.0951 perplex:1.09\n",
      "----------\n",
      "[806/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[807/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[808/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[809/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[810/1000] loss:0.0931 perplex:1.09\n",
      "----------\n",
      "[811/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[812/1000] loss:0.0979 perplex:1.09\n",
      "----------\n",
      "[813/1000] loss:0.0910 perplex:1.09\n",
      "----------\n",
      "[814/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[815/1000] loss:0.0953 perplex:1.09\n",
      "----------\n",
      "[816/1000] loss:0.0979 perplex:1.09\n",
      "----------\n",
      "[817/1000] loss:0.0988 perplex:1.09\n",
      "----------\n",
      "[818/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[819/1000] loss:0.0948 perplex:1.09\n",
      "----------\n",
      "[820/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[821/1000] loss:0.0926 perplex:1.09\n",
      "----------\n",
      "[822/1000] loss:0.1009 perplex:1.09\n",
      "----------\n",
      "[823/1000] loss:0.0934 perplex:1.09\n",
      "----------\n",
      "[824/1000] loss:0.0953 perplex:1.09\n",
      "----------\n",
      "[825/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[826/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[827/1000] loss:0.0950 perplex:1.09\n",
      "----------\n",
      "[828/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[829/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[830/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[831/1000] loss:0.0983 perplex:1.09\n",
      "----------\n",
      "[832/1000] loss:0.0987 perplex:1.09\n",
      "----------\n",
      "[833/1000] loss:0.0907 perplex:1.09\n",
      "----------\n",
      "[834/1000] loss:0.0951 perplex:1.09\n",
      "----------\n",
      "[835/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[836/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[837/1000] loss:0.0953 perplex:1.09\n",
      "----------\n",
      "[838/1000] loss:0.0987 perplex:1.09\n",
      "----------\n",
      "[839/1000] loss:0.0932 perplex:1.09\n",
      "----------\n",
      "[840/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[841/1000] loss:0.0981 perplex:1.09\n",
      "----------\n",
      "[842/1000] loss:0.0945 perplex:1.09\n",
      "----------\n",
      "[843/1000] loss:0.0957 perplex:1.09\n",
      "----------\n",
      "[844/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[845/1000] loss:0.0950 perplex:1.09\n",
      "----------\n",
      "[846/1000] loss:0.0968 perplex:1.09\n",
      "----------\n",
      "[847/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[848/1000] loss:0.0935 perplex:1.09\n",
      "----------\n",
      "[849/1000] loss:0.0951 perplex:1.09\n",
      "----------\n",
      "[850/1000] loss:0.0935 perplex:1.09\n",
      "----------\n",
      "[851/1000] loss:0.0939 perplex:1.09\n",
      "----------\n",
      "[852/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[853/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[854/1000] loss:0.0973 perplex:1.09\n",
      "----------\n",
      "[855/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[856/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[857/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[858/1000] loss:0.0972 perplex:1.09\n",
      "----------\n",
      "[859/1000] loss:0.0899 perplex:1.09\n",
      "----------\n",
      "[860/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[861/1000] loss:0.0940 perplex:1.09\n",
      "----------\n",
      "[862/1000] loss:0.0931 perplex:1.09\n",
      "----------\n",
      "[863/1000] loss:0.1028 perplex:1.09\n",
      "----------\n",
      "[864/1000] loss:0.0927 perplex:1.09\n",
      "----------\n",
      "[865/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[866/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[867/1000] loss:0.0976 perplex:1.09\n",
      "----------\n",
      "[868/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[869/1000] loss:0.0950 perplex:1.09\n",
      "----------\n",
      "[870/1000] loss:0.0927 perplex:1.09\n",
      "----------\n",
      "[871/1000] loss:0.0936 perplex:1.09\n",
      "----------\n",
      "[872/1000] loss:0.0956 perplex:1.09\n",
      "----------\n",
      "[873/1000] loss:0.0914 perplex:1.09\n",
      "----------\n",
      "[874/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[875/1000] loss:0.1010 perplex:1.09\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[876/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[877/1000] loss:0.0987 perplex:1.09\n",
      "----------\n",
      "[878/1000] loss:0.0938 perplex:1.09\n",
      "----------\n",
      "[879/1000] loss:0.0950 perplex:1.09\n",
      "----------\n",
      "[880/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[881/1000] loss:0.0934 perplex:1.09\n",
      "----------\n",
      "[882/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[883/1000] loss:0.0898 perplex:1.09\n",
      "----------\n",
      "[884/1000] loss:0.0905 perplex:1.09\n",
      "----------\n",
      "[885/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[886/1000] loss:0.0977 perplex:1.09\n",
      "----------\n",
      "[887/1000] loss:0.0933 perplex:1.09\n",
      "----------\n",
      "[888/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[889/1000] loss:0.0973 perplex:1.09\n",
      "----------\n",
      "[890/1000] loss:0.0978 perplex:1.09\n",
      "----------\n",
      "[891/1000] loss:0.0986 perplex:1.09\n",
      "----------\n",
      "[892/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[893/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[894/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[895/1000] loss:0.0975 perplex:1.09\n",
      "----------\n",
      "[896/1000] loss:0.0967 perplex:1.09\n",
      "----------\n",
      "[897/1000] loss:0.0985 perplex:1.09\n",
      "----------\n",
      "[898/1000] loss:0.0943 perplex:1.09\n",
      "----------\n",
      "[899/1000] loss:0.0936 perplex:1.09\n",
      "----------\n",
      "[900/1000] loss:0.0967 perplex:1.09\n",
      "----------\n",
      "[901/1000] loss:0.0989 perplex:1.09\n",
      "----------\n",
      "[902/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[903/1000] loss:0.0951 perplex:1.09\n",
      "----------\n",
      "[904/1000] loss:0.0924 perplex:1.09\n",
      "----------\n",
      "[905/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[906/1000] loss:0.0913 perplex:1.09\n",
      "----------\n",
      "[907/1000] loss:0.0960 perplex:1.09\n",
      "----------\n",
      "[908/1000] loss:0.0950 perplex:1.09\n",
      "----------\n",
      "[909/1000] loss:0.0945 perplex:1.09\n",
      "----------\n",
      "[910/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[911/1000] loss:0.0935 perplex:1.09\n",
      "----------\n",
      "[912/1000] loss:0.0964 perplex:1.09\n",
      "----------\n",
      "[913/1000] loss:0.0942 perplex:1.09\n",
      "----------\n",
      "[914/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[915/1000] loss:0.0945 perplex:1.09\n",
      "----------\n",
      "[916/1000] loss:0.0962 perplex:1.09\n",
      "----------\n",
      "[917/1000] loss:0.0941 perplex:1.09\n",
      "----------\n",
      "[918/1000] loss:0.0910 perplex:1.09\n",
      "----------\n",
      "[919/1000] loss:0.1015 perplex:1.09\n",
      "----------\n",
      "[920/1000] loss:0.0952 perplex:1.09\n",
      "----------\n",
      "[921/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[922/1000] loss:0.0970 perplex:1.09\n",
      "----------\n",
      "[923/1000] loss:0.0957 perplex:1.09\n",
      "----------\n",
      "[924/1000] loss:0.0897 perplex:1.09\n",
      "----------\n",
      "[925/1000] loss:0.0910 perplex:1.09\n",
      "----------\n",
      "[926/1000] loss:0.0909 perplex:1.09\n",
      "----------\n",
      "[927/1000] loss:0.0925 perplex:1.09\n",
      "----------\n",
      "[928/1000] loss:0.0957 perplex:1.09\n",
      "----------\n",
      "[929/1000] loss:0.0985 perplex:1.09\n",
      "----------\n",
      "[930/1000] loss:0.0978 perplex:1.09\n",
      "----------\n",
      "[931/1000] loss:0.1003 perplex:1.09\n",
      "----------\n",
      "[932/1000] loss:0.0972 perplex:1.09\n",
      "----------\n",
      "[933/1000] loss:0.0936 perplex:1.09\n",
      "----------\n",
      "[934/1000] loss:0.0933 perplex:1.09\n",
      "----------\n",
      "[935/1000] loss:0.0994 perplex:1.09\n",
      "----------\n",
      "[936/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[937/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[938/1000] loss:0.0954 perplex:1.09\n",
      "----------\n",
      "[939/1000] loss:0.0934 perplex:1.09\n",
      "----------\n",
      "[940/1000] loss:0.0989 perplex:1.09\n",
      "----------\n",
      "[941/1000] loss:0.0948 perplex:1.09\n",
      "----------\n",
      "[942/1000] loss:0.0922 perplex:1.09\n",
      "----------\n",
      "[943/1000] loss:0.0932 perplex:1.09\n",
      "----------\n",
      "[944/1000] loss:0.0979 perplex:1.09\n",
      "----------\n",
      "[945/1000] loss:0.0959 perplex:1.09\n",
      "----------\n",
      "[946/1000] loss:0.0961 perplex:1.09\n",
      "----------\n",
      "[947/1000] loss:0.1002 perplex:1.09\n",
      "----------\n",
      "[948/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[949/1000] loss:0.0932 perplex:1.09\n",
      "----------\n",
      "[950/1000] loss:0.0950 perplex:1.09\n",
      "----------\n",
      "[951/1000] loss:0.1001 perplex:1.09\n",
      "----------\n",
      "[952/1000] loss:0.0941 perplex:1.09\n",
      "----------\n",
      "[953/1000] loss:0.0921 perplex:1.09\n",
      "----------\n",
      "[954/1000] loss:0.0911 perplex:1.09\n",
      "----------\n",
      "[955/1000] loss:0.0987 perplex:1.09\n",
      "----------\n",
      "[956/1000] loss:0.0951 perplex:1.09\n",
      "----------\n",
      "[957/1000] loss:0.0897 perplex:1.09\n",
      "----------\n",
      "[958/1000] loss:0.0934 perplex:1.09\n",
      "----------\n",
      "[959/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[960/1000] loss:0.0958 perplex:1.09\n",
      "----------\n",
      "[961/1000] loss:0.0938 perplex:1.09\n",
      "----------\n",
      "[962/1000] loss:0.0946 perplex:1.09\n",
      "----------\n",
      "[963/1000] loss:0.0930 perplex:1.09\n",
      "----------\n",
      "[964/1000] loss:0.0931 perplex:1.09\n",
      "----------\n",
      "[965/1000] loss:0.0935 perplex:1.09\n",
      "----------\n",
      "[966/1000] loss:0.0896 perplex:1.09\n",
      "----------\n",
      "[967/1000] loss:0.1011 perplex:1.09\n",
      "----------\n",
      "[968/1000] loss:0.0963 perplex:1.09\n",
      "----------\n",
      "[969/1000] loss:0.0897 perplex:1.09\n",
      "----------\n",
      "[970/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[971/1000] loss:0.0922 perplex:1.09\n",
      "----------\n",
      "[972/1000] loss:0.0957 perplex:1.09\n",
      "----------\n",
      "[973/1000] loss:0.0925 perplex:1.09\n",
      "----------\n",
      "[974/1000] loss:0.0947 perplex:1.09\n",
      "----------\n",
      "[975/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[976/1000] loss:0.0988 perplex:1.09\n",
      "----------\n",
      "[977/1000] loss:0.0982 perplex:1.09\n",
      "----------\n",
      "[978/1000] loss:0.0923 perplex:1.09\n",
      "----------\n",
      "[979/1000] loss:0.0949 perplex:1.09\n",
      "----------\n",
      "[980/1000] loss:0.0988 perplex:1.09\n",
      "----------\n",
      "[981/1000] loss:0.0974 perplex:1.09\n",
      "----------\n",
      "[982/1000] loss:0.0905 perplex:1.09\n",
      "----------\n",
      "[983/1000] loss:0.0980 perplex:1.09\n",
      "----------\n",
      "[984/1000] loss:0.0908 perplex:1.09\n",
      "----------\n",
      "[985/1000] loss:0.0936 perplex:1.09\n",
      "----------\n",
      "[986/1000] loss:0.0971 perplex:1.09\n",
      "----------\n",
      "[987/1000] loss:0.0969 perplex:1.09\n",
      "----------\n",
      "[988/1000] loss:0.0928 perplex:1.09\n",
      "----------\n",
      "[989/1000] loss:0.0965 perplex:1.09\n",
      "----------\n",
      "[990/1000] loss:0.0957 perplex:1.10\n",
      "----------\n",
      "[991/1000] loss:0.8743 perplex:2.24\n",
      "----------\n",
      "[992/1000] loss:1.5598 perplex:4.38\n",
      "----------\n",
      "[993/1000] loss:1.4939 perplex:3.78\n",
      "----------\n",
      "[994/1000] loss:1.1168 perplex:2.91\n",
      "----------\n",
      "[995/1000] loss:0.8185 perplex:2.11\n",
      "----------\n",
      "[996/1000] loss:0.5703 perplex:1.74\n",
      "----------\n",
      "[997/1000] loss:0.4140 perplex:1.46\n",
      "----------\n",
      "[998/1000] loss:0.3027 perplex:1.32\n",
      "----------\n",
      "[999/1000] loss:0.2259 perplex:1.24\n",
      "----------\n",
      "[1000/1000] loss:0.1806 perplex:1.18\n"
     ]
    }
   ],
   "source": [
    "perplexities = trainIter(model, train_Xs, optimizer, criterion, scheduler, num_epochs=1000, print_every=200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "谁人国临天，俱门醉嗟侯客。君来必知宴颜成，相草弦入茫茫。\n"
     ]
    }
   ],
   "source": [
    "randomDemo(model, lang, '谁'.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
